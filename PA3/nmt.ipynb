{"cells":[{"cell_type":"markdown","metadata":{"id":"TjPTaRB4mpCd"},"source":["# Colab FAQ\n","\n","For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n","\n","You need to use the colab GPU for this assignment by selecting:\n","\n","> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"]},{"cell_type":"markdown","metadata":{"id":"s9IS9B9-yUU5"},"source":["# Setup PyTorch\n","\n","All files will be stored at /content/csc421/a3/ folder\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3385,"status":"ok","timestamp":1647892648327,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"Z-6MQhMOlHXD","outputId":"233d0eae-5ac4-4d09-a90b-fb40e2979dcd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","/content/content/csc421/a3\n"]}],"source":["######################################################################\n","# Setup python environment and change the current working directory\n","######################################################################\n","!pip install Pillow\n","%mkdir -p ./content/csc421/a3/\n","%cd ./content/csc421/a3"]},{"cell_type":"markdown","metadata":{"id":"9DaTdRNuUra7"},"source":["# Helper code"]},{"cell_type":"markdown","metadata":{"id":"4BIpGwANoQOg"},"source":["## Utility functions"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6367,"status":"ok","timestamp":1647892658633,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"D-UJHBYZkh7f"},"outputs":[],"source":["%matplotlib inline\n","\n","import os\n","import pdb\n","import argparse\n","import pickle as pkl\n","from pathlib import Path\n","\n","from collections import defaultdict\n","\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from six.moves.urllib.request import urlretrieve\n","import tarfile\n","import pickle\n","import sys\n","\n","\n","def get_file(\n","    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n","):\n","    datadir = os.path.join(cache_dir)\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","\n","    if untar:\n","        untar_fpath = os.path.join(datadir, fname)\n","        fpath = untar_fpath + \".tar.gz\"\n","    else:\n","        fpath = os.path.join(datadir, fname)\n","\n","    print(fpath)\n","    if not os.path.exists(fpath):\n","        print(\"Downloading data from\", origin)\n","\n","        error_msg = \"URL fetch failure on {}: {} -- {}\"\n","        try:\n","            try:\n","                urlretrieve(origin, fpath)\n","            except URLError as e:\n","                raise Exception(error_msg.format(origin, e.errno, e.reason))\n","            except HTTPError as e:\n","                raise Exception(error_msg.format(origin, e.code, e.msg))\n","        except (Exception, KeyboardInterrupt) as e:\n","            if os.path.exists(fpath):\n","                os.remove(fpath)\n","            raise\n","\n","    if untar:\n","        if not os.path.exists(untar_fpath):\n","            print(\"Extracting file.\")\n","            with tarfile.open(fpath) as archive:\n","                archive.extractall(datadir)\n","        return untar_fpath\n","\n","    if extract:\n","        _extract_archive(fpath, datadir, archive_format)\n","\n","    return fpath\n","\n","\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","\n","\n","def to_var(tensor, cuda):\n","    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n","\n","    Arguments:\n","        tensor: A Tensor object.\n","        cuda: A boolean flag indicating whether to use the GPU.\n","\n","    Returns:\n","        A Variable object, on the GPU if cuda==True.\n","    \"\"\"\n","    if cuda:\n","        return Variable(tensor.cuda())\n","    else:\n","        return Variable(tensor)\n","\n","\n","def create_dir_if_not_exists(directory):\n","    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","def save_loss_plot(train_losses, val_losses, opts):\n","    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n","    plt.figure()\n","    plt.plot(range(len(train_losses)), train_losses)\n","    plt.plot(range(len(val_losses)), val_losses)\n","    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n","    plt.xlabel(\"Epochs\", fontsize=16)\n","    plt.ylabel(\"Loss\", fontsize=16)\n","    plt.xticks(fontsize=14)\n","    plt.yticks(fontsize=14)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n","    plt.close()\n","\n","\n","def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n","    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n","\n","    Arguments:\n","        l1: Tuple of lists containing training / val losses for model 1.\n","        l2: Tuple of lists containing training / val losses for model 2.\n","        o1: Options for model 1.\n","        o2: Options for model 2.\n","        fn: Output file name.\n","        s: Number of training iterations to average over.\n","    \"\"\"\n","    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n","    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n","\n","    plt.figure()\n","\n","    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n","\n","    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n","    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n","    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n","\n","    # Validation losses are assumed to be by epoch\n","    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n","    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n","    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n","\n","    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n","    ax[0].set_ylabel(\"Loss\", fontsize=10)\n","    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n","    ax[1].set_ylabel(\"Loss\", fontsize=10)\n","    ax[0].legend(loc=\"upper right\")\n","    ax[1].legend(loc=\"upper right\")\n","\n","    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n","    plt.tight_layout()\n","    fig.subplots_adjust(top=0.85)\n","    plt.legend()\n","\n","    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n","    plt.savefig(plt_path)\n","    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n","\n","\n","def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n","    \"\"\"Plot comparison of training and validation loss curves from all four\n","    runs in Part 3, comparing by dataset while holding hidden size constant.\n","\n","    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n","\n","    Arguments:\n","        l1: Tuple of lists containing training / val losses for model 1.\n","        l2: Tuple of lists containing training / val losses for model 2.\n","        l3: Tuple of lists containing training / val losses for model 3.\n","        l4: Tuple of lists containing training / val losses for model 4.\n","        o1: Options for model 1.\n","        o2: Options for model 2.\n","        o3: Options for model 3.\n","        o4: Options for model 4.\n","        fn: Output file name.\n","        s: Number of training iterations to average over.\n","    \"\"\"\n","    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n","    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n","    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n","    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n","\n","    plt.figure()\n","    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n","\n","    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n","    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n","    ax[0][0].title.set_text(\n","        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n","    )\n","\n","    # Validation losses are assumed to be by epoch\n","    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n","    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n","    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n","\n","    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n","    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n","    ax[1][0].title.set_text(\n","        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n","    )\n","\n","    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n","    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n","    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n","\n","    for i in range(2):\n","        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n","        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n","        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][0].legend(loc=\"upper right\")\n","        ax[i][1].legend(loc=\"upper right\")\n","\n","    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n","    plt.tight_layout()\n","    fig.subplots_adjust(top=0.9)\n","    plt.legend()\n","    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n","    plt.close()\n","\n","\n","def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n","    \"\"\"Plot comparison of training and validation loss curves from all four\n","    runs in Part 3, comparing by hidden size while holding dataset constant.\n","\n","    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n","\n","    Arguments:\n","        l1: Tuple of lists containing training / val losses for model 1.\n","        l2: Tuple of lists containing training / val losses for model 2.\n","        l3: Tuple of lists containing training / val losses for model 3.\n","        l4: Tuple of lists containing training / val losses for model 4.\n","        o1: Options for model 1.\n","        o2: Options for model 2.\n","        o3: Options for model 3.\n","        o4: Options for model 4.\n","        fn: Output file name.\n","        s: Number of training iterations to average over.\n","    \"\"\"\n","    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n","    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n","    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n","    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n","\n","    plt.figure()\n","    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n","\n","    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n","    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n","    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n","\n","    # Validation losses are assumed to be by epoch\n","    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n","    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n","    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n","\n","    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n","    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n","    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n","\n","    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n","    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n","    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n","\n","    for i in range(2):\n","        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n","        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n","        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][0].legend(loc=\"upper right\")\n","        ax[i][1].legend(loc=\"upper right\")\n","\n","    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n","    plt.tight_layout()\n","    fig.subplots_adjust(top=0.9)\n","    plt.legend()\n","    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n","    plt.close()\n","\n","\n","def checkpoint(encoder, decoder, idx_dict, opts):\n","    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n","    contains the char_to_index and index_to_char mappings, and the start_token\n","    and end_token values.\n","    \"\"\"\n","    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n","        torch.save(encoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n","        torch.save(decoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n","        pkl.dump(idx_dict, f)"]},{"cell_type":"markdown","metadata":{"id":"pbvpn4MaV0I1"},"source":["## Data loader"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1647892658634,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"XVT4TNTOV3Eg"},"outputs":[],"source":["def read_lines(filename):\n","    \"\"\"Read a file and split it into lines.\"\"\"\n","    lines = open(filename).read().strip().lower().split(\"\\n\")\n","    return lines\n","\n","\n","def read_pairs(filename):\n","    \"\"\"Reads lines that consist of two words, separated by a space.\n","\n","    Returns:\n","        source_words: A list of the first word in each line of the file.\n","        target_words: A list of the second word in each line of the file.\n","    \"\"\"\n","    lines = read_lines(filename)\n","    source_words, target_words = [], []\n","    for line in lines:\n","        line = line.strip()\n","        if line:\n","            source, target = line.split()\n","            source_words.append(source)\n","            target_words.append(target)\n","    return source_words, target_words\n","\n","\n","def all_alpha_or_dash(s):\n","    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n","    return all(c.isalpha() or c == \"-\" for c in s)\n","\n","\n","def filter_lines(lines):\n","    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n","    return [line for line in lines if all_alpha_or_dash(line)]\n","\n","\n","def load_data(file_name):\n","    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n","    path = \"./data/{}.txt\".format(file_name)\n","    source_lines, target_lines = read_pairs(path)\n","\n","    # Filter lines\n","    source_lines = filter_lines(source_lines)\n","    target_lines = filter_lines(target_lines)\n","\n","    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n","\n","    # Create a dictionary mapping each character to a unique index\n","    char_to_index = {\n","        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n","    }\n","\n","    # Add start and end tokens to the dictionary\n","    start_token = len(char_to_index)\n","    end_token = len(char_to_index) + 1\n","    char_to_index[\"SOS\"] = start_token\n","    char_to_index[\"EOS\"] = end_token\n","\n","    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n","    index_to_char = {index: char for (char, index) in char_to_index.items()}\n","\n","    # Store the final size of the vocabulary\n","    vocab_size = len(char_to_index)\n","\n","    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n","\n","    idx_dict = {\n","        \"char_to_index\": char_to_index,\n","        \"index_to_char\": index_to_char,\n","        \"start_token\": start_token,\n","        \"end_token\": end_token,\n","    }\n","\n","    return line_pairs, vocab_size, idx_dict\n","\n","\n","def create_dict(pairs):\n","    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n","    This is used to make batches: each batch consists of two parallel tensors, one containing\n","    all source indexes and the other containing all corresponding target indexes.\n","    Within a batch, all the source words are the same length, and all the target words are\n","    the same length.\n","    \"\"\"\n","    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n","\n","    d = defaultdict(list)\n","    for (s, t) in unique_pairs:\n","        d[(len(s), len(t))].append((s, t))\n","\n","    return d"]},{"cell_type":"markdown","metadata":{"id":"bRWfRdmVVjUl"},"source":["## Training and evaluation code"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":984,"status":"ok","timestamp":1647892659603,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"wa5-onJhoSeM"},"outputs":[],"source":["def string_to_index_list(s, char_to_index, end_token):\n","    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n","    return [char_to_index[char] for char in s] + [\n","        end_token\n","    ]  # Adds the end token to each index list\n","\n","\n","def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n","    words (whitespace-separated), running the encoder-decoder model to translate each\n","    word independently, and then stitching the words back together with spaces between them.\n","    \"\"\"\n","    if idx_dict is None:\n","        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n","    return \" \".join(\n","        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n","    )\n","\n","\n","def translate(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n","\n","    char_to_index = idx_dict[\"char_to_index\"]\n","    index_to_char = idx_dict[\"index_to_char\"]\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","\n","    max_generated_chars = 20\n","    gen_string = \"\"\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(\n","        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n","    )  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_last_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_last_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    for i in range(max_generated_chars):\n","        ## slow decoding, recompute everything at each time\n","        decoder_outputs, attention_weights = decoder(\n","            decoder_inputs, encoder_annotations, decoder_hidden\n","        )\n","\n","        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","        ni = ni[-1]  # latest output token\n","\n","        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","\n","        if ni == end_token:\n","            break\n","        else:\n","            gen_string = \"\".join(\n","                [\n","                    index_to_char[int(item)]\n","                    for item in generated_words.cpu().numpy().reshape(-1)\n","                ]\n","            )\n","\n","    return gen_string\n","\n","\n","def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n","    if idx_dict is None:\n","        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n","    char_to_index = idx_dict[\"char_to_index\"]\n","    index_to_char = idx_dict[\"index_to_char\"]\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","\n","    max_generated_chars = 20\n","    gen_string = \"\"\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(\n","        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n","    )  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    produced_end_token = False\n","\n","    for i in range(max_generated_chars):\n","        ## slow decoding, recompute everything at each time\n","        decoder_outputs, attention_weights = decoder(\n","            decoder_inputs, encoder_annotations, decoder_hidden\n","        )\n","        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","        ni = ni[-1]  # latest output token\n","\n","        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","\n","        if ni == end_token:\n","            break\n","        else:\n","            gen_string = \"\".join(\n","                [\n","                    index_to_char[int(item)]\n","                    for item in generated_words.cpu().numpy().reshape(-1)\n","                ]\n","            )\n","\n","    if isinstance(attention_weights, tuple):\n","        ## transformer's attention mweights\n","        attention_weights, self_attention_weights = attention_weights\n","\n","    all_attention_weights = attention_weights.data.cpu().numpy()\n","\n","    for i in range(len(all_attention_weights)):\n","        attention_weights_matrix = all_attention_weights[i].squeeze()\n","        fig = plt.figure()\n","        ax = fig.add_subplot(111)\n","        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n","        fig.colorbar(cax)\n","\n","        # Set up axes\n","        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n","        ax.set_xticklabels(\n","            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n","        )\n","\n","        # Show label at every tick\n","        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","        # Add title\n","        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n","        plt.tight_layout()\n","        plt.grid(\"off\")\n","        plt.show()\n","\n","    return gen_string\n","\n","\n","def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n","    \"\"\"Train/Evaluate the model on a dataset.\n","\n","    Arguments:\n","        data_dict: The validation/test word pairs, organized by source and target lengths.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n","        opts: The command-line arguments.\n","\n","    Returns:\n","        mean_loss: The average loss over all batches from data_dict.\n","    \"\"\"\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","    char_to_index = idx_dict[\"char_to_index\"]\n","\n","    losses = []\n","    for key in data_dict:\n","        input_strings, target_strings = zip(*data_dict[key])\n","        input_tensors = [\n","            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n","            for s in input_strings\n","        ]\n","        target_tensors = [\n","            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n","            for s in target_strings\n","        ]\n","\n","        num_tensors = len(input_tensors)\n","        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n","\n","        for i in range(num_batches):\n","\n","            start = i * opts.batch_size\n","            end = start + opts.batch_size\n","\n","            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n","            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n","\n","            # The batch size may be different in each epoch\n","            BS = inputs.size(0)\n","\n","            encoder_annotations, encoder_hidden = encoder(inputs)\n","\n","            # The last hidden state of the encoder becomes the first hidden state of the decoder\n","            decoder_hidden = encoder_hidden\n","\n","            start_vector = (\n","                torch.ones(BS).long().unsqueeze(1) * start_token\n","            )  # BS x 1 --> 16x1  CHECKED\n","            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n","\n","            loss = 0.0\n","\n","            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n","\n","            decoder_inputs = torch.cat(\n","                [decoder_input, targets[:, 0:-1]], dim=1\n","            )  # Gets decoder inputs by shifting the targets to the right\n","\n","            decoder_outputs, attention_weights = decoder(\n","                decoder_inputs, encoder_annotations, decoder_hidden\n","            )\n","            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n","            targets_flatten = targets.view(-1)\n","\n","            loss = criterion(decoder_outputs_flatten, targets_flatten)\n","\n","            losses.append(loss.item())\n","\n","            ## training if an optimizer is provided\n","            if optimizer:\n","                # Zero gradients\n","                optimizer.zero_grad()\n","                # Compute gradients\n","                loss.backward()\n","                # Update the parameters of the encoder and decoder\n","                optimizer.step()\n","\n","    return losses\n","\n","\n","def training_loop(\n","    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n","):\n","    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n","        * Prints training and val loss each epoch.\n","        * Prints qualitative translation results each epoch using TEST_SENTENCE\n","        * Saves an attention map for TEST_WORD_ATTN each epoch\n","        * Returns loss curves for comparison\n","\n","    Arguments:\n","        train_dict: The training word pairs, organized by source and target lengths.\n","        val_dict: The validation word pairs, organized by source and target lengths.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n","        opts: The command-line arguments.\n","\n","    Returns:\n","        losses: Lists containing training and validation loss curves.\n","    \"\"\"\n","\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","    char_to_index = idx_dict[\"char_to_index\"]\n","\n","    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n","\n","    best_val_loss = 1e6\n","    train_losses = []\n","    val_losses = []\n","\n","    mean_train_losses = []\n","    mean_val_losses = []\n","\n","    early_stopping_counter = 0\n","\n","    for epoch in range(opts.nepochs):\n","\n","        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n","\n","        train_loss = compute_loss(\n","            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n","        )\n","        val_loss = compute_loss(\n","            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n","        )\n","\n","        mean_train_loss = np.mean(train_loss)\n","        mean_val_loss = np.mean(val_loss)\n","\n","        if mean_val_loss < best_val_loss:\n","            checkpoint(encoder, decoder, idx_dict, opts)\n","            best_val_loss = mean_val_loss\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","\n","        if early_stopping_counter > opts.early_stopping_patience:\n","            print(\n","                \"Validation loss has not improved in {} epochs, stopping early\".format(\n","                    opts.early_stopping_patience\n","                )\n","            )\n","            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n","            return (train_losses, mean_val_losses)\n","\n","        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n","        print(\n","            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n","                epoch, mean_train_loss, mean_val_loss, gen_string\n","            )\n","        )\n","\n","        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n","        loss_log.flush()\n","\n","        train_losses += train_loss\n","        val_losses += val_loss\n","\n","        mean_train_losses.append(mean_train_loss)\n","        mean_val_losses.append(mean_val_loss)\n","\n","        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n","\n","    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n","    return (train_losses, mean_val_losses)\n","\n","\n","def print_data_stats(line_pairs, vocab_size, idx_dict):\n","    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n","    print(\"=\" * 80)\n","    print(\"Data Stats\".center(80))\n","    print(\"-\" * 80)\n","    for pair in line_pairs[:5]:\n","        print(pair)\n","    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n","    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n","    print(\"Vocab size: {}\".format(vocab_size))\n","    print(\"=\" * 80)\n","\n","\n","def train(opts):\n","    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n","    print_data_stats(line_pairs, vocab_size, idx_dict)\n","\n","    # Split the line pairs into an 80% train and 20% val split\n","    num_lines = len(line_pairs)\n","    num_train = int(0.8 * num_lines)\n","    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n","\n","    # Group the data by the lengths of the source and target words, to form batches\n","    train_dict = create_dict(train_pairs)\n","    val_dict = create_dict(val_pairs)\n","\n","    ##########################################################################\n","    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n","    ##########################################################################\n","    if opts.encoder_type == \"rnn\":\n","        encoder = GRUEncoder(\n","            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n","        )\n","    elif opts.encoder_type == \"transformer\":\n","        encoder = TransformerEncoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            num_layers=opts.num_transformer_layers,\n","            opts=opts,\n","        )\n","    elif opts.encoder_type == \"attention\":\n","      encoder = AttentionEncoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            opts=opts,\n","        )\n","    else:\n","        raise NotImplementedError\n","\n","    if opts.decoder_type == \"rnn\":\n","        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n","    elif opts.decoder_type == \"rnn_attention\":\n","        decoder = RNNAttentionDecoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            attention_type=opts.attention_type,\n","        )\n","    elif opts.decoder_type == \"transformer\":\n","        decoder = TransformerDecoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            num_layers=opts.num_transformer_layers,\n","        )\n","    elif opts.encoder_type == \"attention\":\n","      decoder = AttentionDecoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","        )\n","    else:\n","        raise NotImplementedError\n","\n","    #### setup checkpoint path\n","    model_name = \"h{}-bs{}-{}-{}\".format(\n","        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n","    )\n","    opts.checkpoint_path = model_name\n","    create_dir_if_not_exists(opts.checkpoint_path)\n","    ####\n","\n","    if opts.cuda:\n","        encoder.cuda()\n","        decoder.cuda()\n","        print(\"Moved models to GPU!\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(\n","        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n","    )\n","\n","    try:\n","        losses = training_loop(\n","            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n","        )\n","    except KeyboardInterrupt:\n","        print(\"Exiting early from training.\")\n","        return encoder, decoder, losses\n","\n","    return encoder, decoder, losses\n","\n","\n","def print_opts(opts):\n","    \"\"\"Prints the values of all command-line arguments.\"\"\"\n","    print(\"=\" * 80)\n","    print(\"Opts\".center(80))\n","    print(\"-\" * 80)\n","    for key in opts.__dict__:\n","        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n","    print(\"=\" * 80)"]},{"cell_type":"markdown","metadata":{"id":"0yh08KhgnA30"},"source":["## Download dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1083,"status":"ok","timestamp":1647892660681,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"aROU2xZanDKq","outputId":"25b5fd4d-2363-4dcc-d928-721e2f8cffb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["data/pig_latin_small.txt\n","Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n","data/pig_latin_large.txt\n","Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"]}],"source":["######################################################################\n","# Download Translation datasets\n","######################################################################\n","data_fpath = get_file(\n","    fname=\"pig_latin_small.txt\",\n","    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n","    untar=False,\n",")\n","\n","data_fpath = get_file(\n","    fname=\"pig_latin_large.txt\",\n","    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n","    untar=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"YDYMr7NclZdw"},"source":["# Part 1: Neural machine translation (NMT)\n","\n","In this section, you will implement a Gated Recurrent Unit (GRU) cell, a common type of recurrent neural network (RNN). The GRU cell is a simplification of the Long Short-Term Memory cell. Therefore, we have provided you with an implemented LSTM cell (`MyLSTMCell`), which you can reference when completing `MyGRUCell`."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1647892664043,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"cOnALRQkkjDO"},"outputs":[],"source":["class MyLSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyLSTMCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        self.Wif = nn.Linear(input_size, hidden_size)\n","        self.Whf = nn.Linear(hidden_size, hidden_size)\n","\n","        self.Wii = nn.Linear(input_size, hidden_size)\n","        self.Whi = nn.Linear(hidden_size, hidden_size)\n","\n","        self.Wic = nn.Linear(input_size, hidden_size)\n","        self.Whc = nn.Linear(hidden_size, hidden_size)\n","\n","        self.Wio = nn.Linear(input_size, hidden_size)\n","        self.Who = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, x, h_prev, c_prev):\n","        \"\"\"Forward pass of the LSTM computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","            c_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","            c_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n","        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n","\n","        c = torch.tanh(self.Wic(x) + self.Whc(h_prev))\n","        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n","\n","        c_new = f * c_prev + i * c\n","        h_new = o * torch.tanh(c_new)\n","\n","        return h_new, c_new"]},{"cell_type":"markdown","metadata":{"id":"dCae1mOUlZrC"},"source":["## Step 1: GRU Cell\n","Please implement the `MyGRUCell` class defined in the next cell. "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1647892667986,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"DGyxqZIQzTJH"},"outputs":[],"source":["class MyGRUCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyGRUCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # Input linear layers\n","        self.Wiz = nn.Linear(input_size, hidden_size)\n","        self.Wir = nn.Linear(input_size, hidden_size)\n","        self.Wih = nn.Linear(input_size, hidden_size)\n","\n","        # Hidden linear layers\n","        self.Whz = nn.Linear(hidden_size, hidden_size)\n","        self.Whr = nn.Linear(hidden_size, hidden_size)\n","        self.Whh = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, x, h_prev):\n","        \"\"\"Forward pass of the GRU computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n","        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n","        g = torch.tanh(self.Wih(x) + self.Whh(r * h_prev))\n","        h_new = (1 - z) * h_prev + z * g\n","        return h_new"]},{"cell_type":"markdown","metadata":{"id":"ecEq4TP2lZ4Z"},"source":["## Step 2: GRU Encoder\n","\n","The following cells use your `MyGRUCell` implementation to build a recurrent encoder and decoder. Please read the implementations to understand what they do and run the cells before proceeding."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":222,"status":"ok","timestamp":1647892675160,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"8jDNim2fmVJV"},"outputs":[],"source":["class GRUEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(GRUEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.gru = MyGRUCell(hidden_size, hidden_size)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        hidden = self.init_hidden(batch_size)\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","        annotations = []\n","\n","        for i in range(seq_len):\n","            x = encoded[:, i, :]  # Get the current time step, across the whole batch\n","            hidden = self.gru(x, hidden)\n","            annotations.append(hidden)\n","\n","        annotations = torch.stack(annotations, dim=1)\n","        return annotations, hidden\n","\n","    def init_hidden(self, bs):\n","        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n","        of a batch of sequences.\n","\n","        Arguments:\n","            bs: The batch size for the initial hidden state.\n","\n","        Returns:\n","            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n","        \"\"\"\n","        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":314,"status":"ok","timestamp":1647892678488,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"HvwizYM9ma4p"},"outputs":[],"source":["class RNNDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(RNNDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the non-attentional decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch. (batch_size x seq_len)\n","            annotations: This is not used here. It just maintains consistency with the\n","                    interface used by the AttentionDecoder class.\n","            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            None\n","        \"\"\"\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        hiddens = []\n","        h_prev = hidden_init\n","\n","        for i in range(seq_len):\n","            x = embed[\n","                :, i, :\n","            ]  # Get the current time step input tokens, across the whole batch\n","            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n","            hiddens.append(h_prev)\n","\n","        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n","\n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, None"]},{"cell_type":"markdown","metadata":{"id":"TSDTbsydlaGI"},"source":["## Step 3: Training and Analysis\n","\n","Train the encoder-decoder model to perform English --> Pig Latin translation. We will start by training on the smaller dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153129,"status":"ok","timestamp":1647841465364,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"XmVuXTozTPF7","outputId":"83c259b7-36b9-415c-9055-81a655f9f4e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                early_stopping_patience: 20                                     \n","                             batch_size: 64                                     \n","                            hidden_size: 32                                     \n","                           encoder_type: rnn                                    \n","                           decoder_type: rnn                                    \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('temporary', 'emporarytay')\n","('songs', 'ongssay')\n","('esteemed', 'esteemedway')\n","('consequent', 'onsequentcay')\n","('remembering', 'ememberingray')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.296 | Val loss: 2.045 | Gen: ay-ingay-ingay-ingay ay-ingay-ingay-ingay ingay-ingay-ingay-in ingay-ingay-ingay-in ingay-ingay-ingay-in\n","Epoch:   1 | Train loss: 1.870 | Val loss: 1.815 | Gen: eday ay-ationgay-ontingay ongay-ontingay-ontin ingeday oonday\n","Epoch:   2 | Train loss: 1.675 | Val loss: 1.700 | Gen: eay-ontedway andedway ontingway isserway oongay-ontedway\n","Epoch:   3 | Train loss: 1.541 | Val loss: 1.611 | Gen: eay-otay-ontiontay andway ontingway iserway ongay-ondedway\n","Epoch:   4 | Train loss: 1.432 | Val loss: 1.553 | Gen: etay-ationsay andway ontedway issonsay ondedway\n","Epoch:   5 | Train loss: 1.353 | Val loss: 1.500 | Gen: etay arway ontiontay ilway ordendway\n","Epoch:   6 | Train loss: 1.291 | Val loss: 1.478 | Gen: etay arway-ondway ontiontay iway-ondway ondedway\n","Epoch:   7 | Train loss: 1.213 | Val loss: 1.455 | Gen: etay arway-angay-ondway ontionstay iway-ousedway onday-ousedway\n","Epoch:   8 | Train loss: 1.160 | Val loss: 1.437 | Gen: ehay ariway-away ontionstay iway-ousedway ordenay-ousedway\n","Epoch:   9 | Train loss: 1.117 | Val loss: 1.433 | Gen: ehay ariway ontionstay isway ordway\n","Epoch:  10 | Train loss: 1.085 | Val loss: 1.373 | Gen: ehay-atedway ariedway ontiontay isway-aghay-ingay-ot ordgay-ingay-ightnay\n","Epoch:  11 | Train loss: 1.049 | Val loss: 1.367 | Gen: eateay-ousehay ariway ontionstay iway ordway\n","Epoch:  12 | Train loss: 1.003 | Val loss: 1.372 | Gen: ehay ariway ontificay-ightnay isway ordgay-ingay-ousedwa\n","Epoch:  13 | Train loss: 0.973 | Val loss: 1.314 | Gen: ehay ariway ontistnay isway ordday\n","Epoch:  14 | Train loss: 0.944 | Val loss: 1.329 | Gen: ehay ariway ontificationsday isway ordgay-ingay-oushent\n","Epoch:  15 | Train loss: 0.926 | Val loss: 1.307 | Gen: ehay ariway ontionstay iway ordorsway\n","Epoch:  16 | Train loss: 0.900 | Val loss: 1.303 | Gen: ehay ariway ontisgray isway-aghay-ousehay ordorway\n","Epoch:  17 | Train loss: 0.871 | Val loss: 1.257 | Gen: ehay ariway ontisgay-ightnay isway orsingway\n","Epoch:  18 | Train loss: 0.853 | Val loss: 1.300 | Gen: etay airway ongay-ightnay isway ordway\n","Epoch:  19 | Train loss: 0.839 | Val loss: 1.254 | Gen: ehay ariway ontisgay-ightnay isway orndingway\n","Epoch:  20 | Train loss: 0.811 | Val loss: 1.335 | Gen: etay airway ontificationsday isway-aghay-ousehay ordday\n","Epoch:  21 | Train loss: 0.828 | Val loss: 1.346 | Gen: ehay aiway ongay-ithay iwlay orndingway\n","Epoch:  22 | Train loss: 0.816 | Val loss: 1.273 | Gen: ehay ariway ontightay isway ordouslyway\n","Epoch:  23 | Train loss: 0.769 | Val loss: 1.265 | Gen: ehay arifianceway ongay-ortionstay isway orsbay\n","Epoch:  24 | Train loss: 0.750 | Val loss: 1.312 | Gen: ehay airway ondingtray isway ordray\n","Epoch:  25 | Train loss: 0.742 | Val loss: 1.263 | Gen: ehay arifichay onditray isway ordray\n","Epoch:  26 | Train loss: 0.721 | Val loss: 1.260 | Gen: ehay arileway ongay-ightay isway ordgay\n","Epoch:  27 | Train loss: 0.700 | Val loss: 1.275 | Gen: ehay arileway ontidglynay isway ordously\n","Epoch:  28 | Train loss: 0.693 | Val loss: 1.258 | Gen: ehay arificay ontingay-ightnay isway orsbay\n","Epoch:  29 | Train loss: 0.705 | Val loss: 1.330 | Gen: etay airlay ongittray-ybay isway ordgay-onday-ousgray\n","Epoch:  30 | Train loss: 0.704 | Val loss: 1.318 | Gen: ehay airlyway ondingay-ightnay isway orngingnay-oushstay\n","Epoch:  31 | Train loss: 0.703 | Val loss: 1.312 | Gen: ehay ailway onditray isway ordway\n","Epoch:  32 | Train loss: 0.688 | Val loss: 1.254 | Gen: ehay arifilay ontidglynay isway orkingway\n","Epoch:  33 | Train loss: 0.661 | Val loss: 1.263 | Gen: etay aillway ontigray-agstay isway orkngay\n","Epoch:  34 | Train loss: 0.638 | Val loss: 1.243 | Gen: ehay airlyway ontidglynay isway orkway-oodosedway\n","Epoch:  35 | Train loss: 0.620 | Val loss: 1.231 | Gen: ehay arifilway ondingtray isway orkway\n","Epoch:  36 | Train loss: 0.605 | Val loss: 1.238 | Gen: ehtay ariledway ontidglynay isway orkngay\n","Epoch:  37 | Train loss: 0.595 | Val loss: 1.288 | Gen: ehay ailway ontidglynay isway orkway\n","Epoch:  38 | Train loss: 0.597 | Val loss: 1.240 | Gen: ehtay airlyway ondingtray isway orkngay\n","Epoch:  39 | Train loss: 0.586 | Val loss: 1.308 | Gen: ehay arifilay ontingway isway ordfay\n","Epoch:  40 | Train loss: 0.596 | Val loss: 1.297 | Gen: ehay ariledway ontidingway isway orkngay\n","Epoch:  41 | Train loss: 0.592 | Val loss: 1.325 | Gen: ehtay airlyway ontidray isway orkway\n","Epoch:  42 | Train loss: 0.574 | Val loss: 1.258 | Gen: ehay airlyway ontidnestway isway orsbay\n","Epoch:  43 | Train loss: 0.562 | Val loss: 1.304 | Gen: ehtay airlyway onditray isway orsbay\n","Epoch:  44 | Train loss: 0.552 | Val loss: 1.284 | Gen: ehtay airlyway onditray-agthway isway orkway\n","Epoch:  45 | Train loss: 0.547 | Val loss: 1.351 | Gen: ehtay airlyway ondingtray isway ordfay\n","Epoch:  46 | Train loss: 0.551 | Val loss: 1.343 | Gen: ehtay airway ontondingway isway ordfay\n","Epoch:  47 | Train loss: 0.549 | Val loss: 1.295 | Gen: ehtay ailway ontingcay-ousghay isway orkngay\n","Epoch:  48 | Train loss: 0.537 | Val loss: 1.324 | Gen: ehay arilway ontingcay-ousgray isway orsfray\n","Epoch:  49 | Train loss: 0.529 | Val loss: 1.317 | Gen: ehtay airlay ongintray-ooodoutway isway ordray\n","Obtained lowest validation loss of: 1.2309542949809584\n","source:\t\tthe air conditioning is working \n","translated:\tehtay airlay ongintray-ooodoutway isway ordray\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","rnn_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 0.005,\n","    \"lr_decay\": 0.99,\n","    \"early_stopping_patience\": 20,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n","    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n","    \"attention_type\": \"\",   # options: additive / scaled_dot\n","}\n","rnn_args_s.update(args_dict)\n","\n","print_opts(rnn_args_s)\n","rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"0mR97V_NtER6"},"source":["Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n","\n","For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a quick and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169988,"status":"ok","timestamp":1647841635338,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"H3YLrAjsmx_W","outputId":"b950c44a-5fe9-497a-b1e2-9c29609f6474"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_large                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                early_stopping_patience: 10                                     \n","                             batch_size: 512                                    \n","                            hidden_size: 32                                     \n","                           encoder_type: rnn                                    \n","                           decoder_type: rnn                                    \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('interconnect', 'interconnectway')\n","('refreshed', 'efreshedray')\n","('parentheses', 'arenthesespay')\n","('phenomenon', 'enomenonphay')\n","('automotive', 'automotiveway')\n","Num unique word pairs: 22402\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.293 | Val loss: 2.077 | Gen: ay ay ontay-ay ingsay-ay ontay-ay\n","Epoch:   1 | Train loss: 1.886 | Val loss: 1.955 | Gen: eray-ay-ay ay-ay oontay-ay ingsay-ay-ay-ay-ay ontay-ay\n","Epoch:   2 | Train loss: 1.739 | Val loss: 1.855 | Gen: etay-ayday-ayday away-ayday otingsay-ayday ingssay-ayday-ayday- ontay-ayday\n","Epoch:   3 | Train loss: 1.634 | Val loss: 1.792 | Gen: estay-ayday-ayday-ay away-ayday-ayday-ayd otingsway-ayday-ayda ingsray-ingsay-ayday onstay-ayday-ayday-a\n","Epoch:   4 | Train loss: 1.553 | Val loss: 1.750 | Gen: etay-ayday away otingsway-away-ayday ingsray-ayday-ayday ortay-ayday-ayday\n","Epoch:   5 | Train loss: 1.486 | Val loss: 1.717 | Gen: etay away-away onstay-away-away-awa ishesway orstay-ayday-away-aw\n","Epoch:   6 | Train loss: 1.439 | Val loss: 1.732 | Gen: etedway ayday-ayday onshergray-ingway ishesway oushesway\n","Epoch:   7 | Train loss: 1.385 | Val loss: 1.666 | Gen: edway away onsichay-ayday isssay-away-away-awa oushergray-ayday\n","Epoch:   8 | Train loss: 1.335 | Val loss: 1.621 | Gen: etay-ayday away-ayday-awlay-awl onshay-awlay-awlay-a isshay onshay-away-ayday\n","Epoch:   9 | Train loss: 1.292 | Val loss: 1.594 | Gen: eway ayday onsicationshay ishedway oughtway\n","Epoch:  10 | Train loss: 1.252 | Val loss: 1.603 | Gen: etay away onsishedway issionsway --------------------\n","Epoch:  11 | Train loss: 1.230 | Val loss: 1.593 | Gen: etay-ayday aysay-ayday-ayday onsay-ingsray-ayday- issway oorthay-ingsray-ayda\n","Epoch:  12 | Train loss: 1.193 | Val loss: 1.506 | Gen: etway arway otionsicay-ayday isway oughtray-inway\n","Epoch:  13 | Train loss: 1.140 | Val loss: 1.535 | Gen: ethay arway onsicationsay-awlay- isway ourgray-inway\n","Epoch:  14 | Train loss: 1.118 | Val loss: 1.466 | Gen: etedway arway onsicationsay isway ordgray-ingsray\n","Epoch:  15 | Train loss: 1.067 | Val loss: 1.451 | Gen: ethay arway-ybay onsicationsay issway ordgray-inway\n","Epoch:  16 | Train loss: 1.046 | Val loss: 1.440 | Gen: entay aray onintionsway isway ordgay-inway-awlay\n","Epoch:  17 | Train loss: 1.022 | Val loss: 1.429 | Gen: etay aray-iway-ayday onsay-ingsray isway oproughtway\n","Epoch:  18 | Train loss: 0.988 | Val loss: 1.440 | Gen: ethay arway-ybay onsicationstay isway orkgay-inway-awlay\n","Epoch:  19 | Train loss: 0.975 | Val loss: 1.407 | Gen: ethay arway ontionsay-ayday iway-ayday orkgay-orfay\n","Epoch:  20 | Train loss: 0.965 | Val loss: 1.403 | Gen: ethay arway ondingshay iway-ayday okringway\n","Epoch:  21 | Train loss: 0.933 | Val loss: 1.363 | Gen: ethay aray onsicationsway iway-ayday orkgay-inway-awlay\n","Epoch:  22 | Train loss: 0.920 | Val loss: 1.345 | Gen: eteray arway ontionsay isway oprgingway\n","Epoch:  23 | Train loss: 0.899 | Val loss: 1.369 | Gen: ethay arway ondingshay isway orvodighay\n","Epoch:  24 | Train loss: 0.887 | Val loss: 1.343 | Gen: ethay arway oningstay isway oprovgray\n","Epoch:  25 | Train loss: 0.860 | Val loss: 1.361 | Gen: ethay arway onningway isway orvodway\n","Epoch:  26 | Train loss: 0.849 | Val loss: 1.324 | Gen: eteray arway oningtay-away-yearay iway-ayday oprovgray\n","Epoch:  27 | Train loss: 0.841 | Val loss: 1.458 | Gen: ehtay aryway otingingshay-ayday issay ughingway\n","Epoch:  28 | Train loss: 0.890 | Val loss: 1.378 | Gen: etay arway ontay-ablitionsway isway orkbay-inway-awlay\n","Epoch:  29 | Train loss: 0.852 | Val loss: 1.315 | Gen: ethay ariway-ayday oninctoryway isway orprovizay\n","Epoch:  30 | Train loss: 0.799 | Val loss: 1.315 | Gen: ethay ariway-ayday ondinglay-axtay isway orproupshay\n","Epoch:  31 | Train loss: 0.775 | Val loss: 1.248 | Gen: ethay ariway-ayday ontingshay isway orpightway\n","Epoch:  32 | Train loss: 0.752 | Val loss: 1.290 | Gen: ethay ariway ondoningtablingway isway orprouvedway\n","Epoch:  33 | Train loss: 0.743 | Val loss: 1.296 | Gen: ethay ariway-ayday ontinghay-atesway isway orkightroupway\n","Epoch:  34 | Train loss: 0.734 | Val loss: 1.271 | Gen: ethay ariway ontondichationway isway orkgingway\n","Epoch:  35 | Train loss: 0.731 | Val loss: 1.328 | Gen: ethay ariway ondingcay isway orkvizay\n","Epoch:  36 | Train loss: 0.807 | Val loss: 1.479 | Gen: ehay ariway-ay ondinglay isway unkinguredway\n","Epoch:  37 | Train loss: 0.943 | Val loss: 1.376 | Gen: ethay ararway ontongray-etay-arday issay orghay-orway\n","Epoch:  38 | Train loss: 0.817 | Val loss: 1.271 | Gen: ehtway ariway ontonglay-inway-awla isway orproughionway\n","Epoch:  39 | Train loss: 0.738 | Val loss: 1.228 | Gen: ehtway ariway ontingtay-inway-awla isway uprowhingway\n","Epoch:  40 | Train loss: 0.705 | Val loss: 1.194 | Gen: ethay ariway ontingtay-inway-awla isway orkighway\n","Epoch:  41 | Train loss: 0.685 | Val loss: 1.164 | Gen: ethay ariway ontingtay-inway-awla isway orkingway\n","Epoch:  42 | Train loss: 0.666 | Val loss: 1.219 | Gen: ethay ariway ontingtay-inway-awla isway orkbingway\n","Epoch:  43 | Train loss: 0.660 | Val loss: 1.230 | Gen: ethay ariway ondoningtay-ayeaway isway orpgordingway\n","Epoch:  44 | Train loss: 0.654 | Val loss: 1.181 | Gen: ethay ariway ontingtay-orfay isway orkbingway\n","Epoch:  45 | Train loss: 0.650 | Val loss: 1.260 | Gen: ethay ariway ontingtay-orfay isway orkingshay\n","Epoch:  46 | Train loss: 0.655 | Val loss: 1.279 | Gen: ethay arway ontionscay isway orkbingway\n","Epoch:  47 | Train loss: 0.651 | Val loss: 1.235 | Gen: ethay ariway ontodinghay isway orkbingway\n","Epoch:  48 | Train loss: 0.639 | Val loss: 1.234 | Gen: ethay ariway ontingtay-aturednay isway orkbingway\n","Epoch:  49 | Train loss: 0.633 | Val loss: 1.241 | Gen: ethay ariway ontondicingway isway orkingway\n","Obtained lowest validation loss of: 1.1641187071800232\n","source:\t\tthe air conditioning is working \n","translated:\tethay ariway ontondicingway isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","rnn_args_l = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_large\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 0.005,\n","    \"lr_decay\": 0.99,\n","    \"early_stopping_patience\": 10,\n","    \"batch_size\": 512,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n","    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n","    \"attention_type\": \"\",   # options: additive / scaled_dot\n","}\n","rnn_args_l.update(args_dict)\n","\n","print_opts(rnn_args_l)\n","rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"01HsZ6EItc56"},"source":["The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Are there significant differences in the validation performance of each model? (see follow-up questions in handout)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"elapsed":656,"status":"ok","timestamp":1647841635978,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"Qyk_9-Fwtekj","outputId":"a218d6cc-7a00-4b31-bd7c-c2cac1ad1ac0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Plot saved to: /content/content/csc421/a3/loss_plot_gru.pdf\n"]},{"data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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\n","text/plain":["<Figure size 720x288 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["save_loss_comparison_gru(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, \"gru\")"]},{"cell_type":"markdown","metadata":{"id":"cE4ijaCzneAt"},"source":["Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it (see follow-up questions in handout)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":729,"status":"ok","timestamp":1647841636706,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"WrNnz8W1nULf","outputId":"a93d34c9-3604-4ed6-f686-063610d04173"},"outputs":[{"name":"stdout","output_type":"stream","text":["source:\t\tthe air conditioning is working \n","translated:\tethay ariway ontondicingway isway orkingway\n","source:\t\timplement scaled dot-product attention \n","translated:\timuniedentway aldecay ottay-ingsray atteninetay-ayday\n","source:\t\tthere are significant differences \n","translated:\tererentay arereway iginicationsway ifeterflay-estray-ax\n"]}],"source":["best_encoder = rnn_encode_l  # Replace with rnn_encode_s or rnn_encode_l\n","best_decoder = rnn_decoder_l  # Replace with rnn_decoder_s or rnn_decoder_l\n","best_args = rnn_args_l     # Replace with rnn_args_s or rnn_args_l\n","\n","TEST_SENTENCE = \"the air conditioning is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n","\n","TEST_SENTENCE_1 = \"implement scaled dot-product attention\"\n","translated_1 = translate_sentence(\n","    TEST_SENTENCE_1, best_encoder, best_decoder, None, best_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE_1, translated_1))\n","\n","TEST_SENTENCE_2 = \"there are significant differences\"\n","translated_2 = translate_sentence(\n","    TEST_SENTENCE_2, best_encoder, best_decoder, None, best_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE_2, translated_2))"]},{"cell_type":"markdown","metadata":{"id":"RWwA6OGqlaTq"},"source":["# Part 2: Attention mechanisms"]},{"cell_type":"markdown","metadata":{"id":"AJSafHSAmu_w"},"source":["## Step 1: Additive attention\n","\n","In the next cell, the [additive attention](https://paperswithcode.com/method/additive-attention) mechanism has been implemented for you. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":539,"status":"ok","timestamp":1647892696663,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"AdewEVSMo5jJ"},"outputs":[],"source":["class AdditiveAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(AdditiveAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # A two layer fully-connected network\n","        # hidden_size * 2 --> hidden_size, ReLU, hidden_size --> 1\n","        self.attention_network = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","        )\n","\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the additive attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state. (batch_size x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x 1 x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The attention_weights must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","        batch_size = keys.size(0)\n","        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(\n","            keys\n","        )\n","        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n","        unnormalized_attention = self.attention_network(concat_inputs)\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(2, 1), values)\n","        return context, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"73_p8d5EmvOJ"},"source":["## Step 2: RNN + additive attention\n","\n","In the next cell, a modification of our `RNNDecoder` that makes use of an additive attention mechanism as been implemented for your. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1647892700058,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"RJaABkXrpJSw"},"outputs":[],"source":["class RNNAttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, attention_type=\"scaled_dot\"):\n","        super(RNNAttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.rnn = MyGRUCell(input_size=hidden_size * 2, hidden_size=hidden_size)\n","        if attention_type == \"additive\":\n","            self.attention = AdditiveAttention(hidden_size=hidden_size)\n","        elif attention_type == \"scaled_dot\":\n","            self.attention = ScaledDotAttention(hidden_size=hidden_size)\n","\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        hiddens = []\n","        attentions = []\n","        h_prev = hidden_init\n","\n","        for i in range(seq_len):\n","            embed_current = embed[\n","                :, i, :\n","            ]  # Get the current time step, across the whole batch\n","            context, attention_weights = self.attention(\n","                h_prev, annotations, annotations\n","            )  # batch_size x 1 x hidden_size\n","            embed_and_context = torch.cat(\n","                [embed_current, context.squeeze(1)], dim=1\n","            )  # batch_size x (2*hidden_size)\n","            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size\n","\n","            hiddens.append(h_prev)\n","            attentions.append(attention_weights)\n","\n","        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n","        attentions = torch.cat(attentions, dim=2)  # batch_size x seq_len x seq_len\n","\n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, attentions"]},{"cell_type":"markdown","metadata":{"id":"vYPae08Io1Fi"},"source":["## Step 3: Training and analysis (with additive attention)\n","\n","Now, run the following cell to train our recurrent encoder-decoder model with additive attention. How does it perform compared to the recurrent encoder-decoder model without attention?"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118744,"status":"ok","timestamp":1647892841646,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"ke6t6rCezpZV","outputId":"1efb8926-c393-4f54-d832-b3251b7d77f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                early_stopping_patience: 10                                     \n","                             batch_size: 64                                     \n","                            hidden_size: 64                                     \n","                           encoder_type: rnn                                    \n","                           decoder_type: rnn_attention                          \n","                         attention_type: additive                               \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('interfere', 'interfereway')\n","('several', 'everalsay')\n","('hour', 'ourhay')\n","('read', 'eadray')\n","('sends', 'endssay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 1.993 | Val loss: 1.811 | Gen: ay ilway ionsay-onsay issay-onsay illway-onday\n","Epoch:   1 | Train loss: 1.503 | Val loss: 1.616 | Gen: etetetay ay ontiongay isay ongray\n","Epoch:   2 | Train loss: 1.214 | Val loss: 1.482 | Gen: etay away ontingingnationsay iway ouray\n","Epoch:   3 | Train loss: 1.002 | Val loss: 1.369 | Gen: ethethay iway ondingtingnay isway oudgay-ingway\n","Epoch:   4 | Train loss: 0.829 | Val loss: 1.227 | Gen: etay away ondingway isway oveway\n","Epoch:   5 | Train loss: 0.700 | Val loss: 1.137 | Gen: ethththay arway oncomprintionday isway orway-ourgngway\n","Epoch:   6 | Train loss: 0.608 | Val loss: 1.106 | Gen: etetthay awray-irway ondincationday isway orway-orway-ingway\n","Epoch:   7 | Train loss: 0.496 | Val loss: 0.914 | Gen: ethay airway oncidiontionday isway odgray-oingway\n","Epoch:   8 | Train loss: 0.375 | Val loss: 0.713 | Gen: ethay arway onditingcay isway orkingway\n","Epoch:   9 | Train loss: 0.291 | Val loss: 0.720 | Gen: ethay airway ondingcationgcay isway oringway\n","Epoch:  10 | Train loss: 0.277 | Val loss: 0.790 | Gen: ethay airway ontiongnay isway oringway\n","Epoch:  11 | Train loss: 0.286 | Val loss: 0.630 | Gen: ethay irarway onditingcay isway orkingway\n","Epoch:  12 | Train loss: 0.236 | Val loss: 0.821 | Gen: etay airway ondigingcay iway orway-ignay\n","Epoch:  13 | Train loss: 0.238 | Val loss: 0.594 | Gen: ethhththhay airay onditingcay isway orkingway\n","Epoch:  14 | Train loss: 0.175 | Val loss: 0.518 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  15 | Train loss: 0.118 | Val loss: 0.433 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  16 | Train loss: 0.073 | Val loss: 0.394 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  17 | Train loss: 0.049 | Val loss: 0.449 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  18 | Train loss: 0.043 | Val loss: 0.419 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  19 | Train loss: 0.040 | Val loss: 0.417 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  20 | Train loss: 0.040 | Val loss: 0.490 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  21 | Train loss: 0.047 | Val loss: 0.519 | Gen: ethay airway onditinncay isway orkingway\n","Epoch:  22 | Train loss: 0.090 | Val loss: 0.613 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  23 | Train loss: 0.121 | Val loss: 0.726 | Gen: etay airway ondingicay isway orkingway\n","Epoch:  24 | Train loss: 0.159 | Val loss: 0.627 | Gen: ethay array onditingtcay isway orkingway\n","Epoch:  25 | Train loss: 0.134 | Val loss: 0.543 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  26 | Train loss: 0.107 | Val loss: 0.466 | Gen: ethay airway onditingicitingcay isway orkingway\n","Validation loss has not improved in 10 epochs, stopping early\n","Obtained lowest validation loss of: 0.3938221916090697\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditionway isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","rnn_attn_args = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 0.005,\n","    \"lr_decay\": 0.99,\n","    \"early_stopping_patience\": 10,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 64,\n","    \"encoder_type\": \"rnn\",            # options: rnn / transformer\n","    \"decoder_type\": \"rnn_attention\",  # options: rnn / rnn_attention / transformer\n","    \"attention_type\": \"additive\",     # options: additive / scaled_dot\n","}\n","rnn_attn_args.update(args_dict)\n","\n","print_opts(rnn_attn_args)\n","rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1647892951989,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"},"user_tz":240},"id":"VNVKbLc0ACj_","outputId":"6f106153-2db3-4b55-cdce-90c3706387ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe air conditioning is working \n","translated:\tethay airway onditionway isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"xq7nhsEio1w-"},"source":["## Step 4: Implement scaled dot-product attention\n","\n","In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"d_j3oY3hqsJQ","executionInfo":{"status":"ok","timestamp":1647893140886,"user_tz":240,"elapsed":245,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}}},"outputs":[],"source":["class ScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(ScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(\n","            torch.tensor(self.hidden_size, dtype=torch.float)\n","        )\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        batch_size = keys.size(0)\n","\n","        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n","        k = self.K(keys)\n","        v = self.V(values)\n","\n","        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) * self.scaling_factor\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(2,1), v)\n","        return context, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"unReAOrjo113"},"source":["## Step 5: Implement causal dot-product Attention\n","\n","\n","Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ovigzQffrKqj","executionInfo":{"status":"ok","timestamp":1647893143664,"user_tz":240,"elapsed":274,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}}},"outputs":[],"source":["class CausalScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(CausalScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.neg_inf = torch.tensor(-1e7)\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(\n","            torch.tensor(self.hidden_size, dtype=torch.float)\n","        )\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        batch_size = keys.size(0)   \n","\n","        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n","        k = self.K(keys)\n","        v = self.V(values)\n","\n","        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) * self.scaling_factor\n","        mask = torch.tril(torch.ones_like(unnormalized_attention) * self.neg_inf, diagonal=-1)\n","\n","        attention_weights = self.softmax(unnormalized_attention + mask)\n","        context = torch.bmm(attention_weights.transpose(2,1), v)\n","        \n","        return context, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"ZkjHbtvT6Qxs"},"source":["## Step 6: Attention encoder and decoder\n","\n","The following cells provide an implementation of an encoder and decoder that use a single `ScaledDotAttention` block. Please read through them to understand what they are doing."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"yKGNqUaX6RLO","executionInfo":{"status":"ok","timestamp":1647893153175,"user_tz":240,"elapsed":228,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}}},"outputs":[],"source":["class AttentionEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(AttentionEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.self_attention = ScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","               \n","        self.attention_mlp = nn.Sequential(\n","                                nn.Linear(hidden_size, hidden_size),\n","                                nn.ReLU(),\n","                              )\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder scaled dot attention.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            None: Used to conform to standard encoder return signature.\n","        \"\"\"\n","        batch_size, seq_len = inputs.size()\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        annotations = encoded\n","        new_annotations, self_attention_weights = self.self_attention(\n","            annotations, annotations, annotations\n","        )  # batch_size x seq_len x hidden_size\n","        residual_annotations = annotations + new_annotations\n","        new_annotations = self.attention_mlp(residual_annotations)\n","        annotations = residual_annotations + new_annotations\n","\n","        return annotations, None\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"vDUvtOee7cMy","executionInfo":{"status":"ok","timestamp":1647893156877,"user_tz":240,"elapsed":420,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}}},"outputs":[],"source":["class AttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(AttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.self_attention = CausalScaledDotAttention(\n","                                hidden_size=hidden_size,\n","                                )\n","                \n","        self.decoder_attention = ScaledDotAttention(\n","                                  hidden_size=hidden_size,\n","                                  )\n","                \n","        self.attention_mlp = nn.Sequential(\n","                                nn.Linear(hidden_size, hidden_size),\n","                                nn.ReLU(),\n","                              )\n","                \n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: Not used in the transformer decoder\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        new_contexts, self_attention_weights = self.self_attention(\n","            contexts, contexts, contexts\n","        )  # batch_size x seq_len x hidden_size\n","        residual_contexts = contexts + new_contexts\n","        new_contexts, encoder_attention_weights = self.decoder_attention(\n","            residual_contexts, annotations, annotations\n","        )  # batch_size x seq_len x hidden_size\n","        residual_contexts = residual_contexts + new_contexts\n","        new_contexts = self.attention_mlp(residual_contexts)\n","        contexts = residual_contexts + new_contexts\n","\n","        encoder_attention_weights_list.append(encoder_attention_weights)\n","        self_attention_weights_list.append(self_attention_weights)\n","\n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","\n","        return output, (encoder_attention_weights, self_attention_weights)"]},{"cell_type":"markdown","metadata":{"id":"B7gJLw5t_rnW"},"source":["## Step 7: Training and analysis (single scaled dot-product attention block)\n","\n","Now, train the following model, with an encoder and decoder each composed a single `ScaledDotAttention` block."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"7MOkZonC8T3f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647893243171,"user_tz":240,"elapsed":78567,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}},"outputId":"a8632406-4b2f-4ebb-d815-92155c700ecb"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 100                                    \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 32                                     \n","                           encoder_type: attention                              \n","                           decoder_type: attention                              \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('interfere', 'interfereway')\n","('several', 'everalsay')\n","('hour', 'ourhay')\n","('read', 'eadray')\n","('sends', 'endssay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.919 | Val loss: 2.499 | Gen: ay ay ay ay ay      \n","Epoch:   1 | Train loss: 2.293 | Val loss: 2.238 | Gen: ay ay-ay-ay-ay-ay-ay-ay inay isay ay\n","Epoch:   2 | Train loss: 2.103 | Val loss: 2.099 | Gen: ay ay-ay-ay-ay-ay-ay-ay inay isisay onay\n","Epoch:   3 | Train loss: 1.971 | Val loss: 1.988 | Gen: eay ay ondonday isisay onay\n","Epoch:   4 | Train loss: 1.871 | Val loss: 1.913 | Gen: elelay aray ondondondonday isay ongway\n","Epoch:   5 | Train loss: 1.798 | Val loss: 1.855 | Gen: ehay arararararararararar ondondondonday isasasisasasasasasas ongway\n","Epoch:   6 | Train loss: 1.737 | Val loss: 1.808 | Gen: ehay irararararararararar ondondonday isasasasway ongway\n","Epoch:   7 | Train loss: 1.683 | Val loss: 1.766 | Gen: ehay irararararararararar ondonday isasasway ongay\n","Epoch:   8 | Train loss: 1.634 | Val loss: 1.729 | Gen: ehay irararararararway ondionday isasway ongay\n","Epoch:   9 | Train loss: 1.588 | Val loss: 1.697 | Gen: ehehay irarararway indindindinday isway ongay\n","Epoch:  10 | Train loss: 1.548 | Val loss: 1.665 | Gen: ehehay irararway indindindindinday isway ongay\n","Epoch:  11 | Train loss: 1.510 | Val loss: 1.635 | Gen: elehay irararway ingondindindinday isway ongay\n","Epoch:  12 | Train loss: 1.475 | Val loss: 1.605 | Gen: elehay arway ingondindingonday isway ongay\n","Epoch:  13 | Train loss: 1.441 | Val loss: 1.577 | Gen: elehay arway ingondingonday isway oray\n","Epoch:  14 | Train loss: 1.408 | Val loss: 1.554 | Gen: elehay arway ingondingonday isway oray\n","Epoch:  15 | Train loss: 1.378 | Val loss: 1.533 | Gen: elethay arway ingondingay isway oray\n","Epoch:  16 | Train loss: 1.350 | Val loss: 1.514 | Gen: elethay arway ingondingay isway ooray\n","Epoch:  17 | Train loss: 1.324 | Val loss: 1.497 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  18 | Train loss: 1.300 | Val loss: 1.480 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  19 | Train loss: 1.277 | Val loss: 1.465 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  20 | Train loss: 1.256 | Val loss: 1.452 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  21 | Train loss: 1.236 | Val loss: 1.438 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  22 | Train loss: 1.217 | Val loss: 1.425 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  23 | Train loss: 1.200 | Val loss: 1.413 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  24 | Train loss: 1.184 | Val loss: 1.401 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  25 | Train loss: 1.169 | Val loss: 1.392 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  26 | Train loss: 1.154 | Val loss: 1.379 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  27 | Train loss: 1.140 | Val loss: 1.370 | Gen: ethay arway ingondingay isway ooray\n","Epoch:  28 | Train loss: 1.128 | Val loss: 1.362 | Gen: etetatatatathtatatet arway ingondingay isway ooray\n","Epoch:  29 | Train loss: 1.116 | Val loss: 1.350 | Gen: etetatatatatatetatat arway ingondingay isway ooray\n","Epoch:  30 | Train loss: 1.104 | Val loss: 1.343 | Gen: etetatatatatatetatat arway ingondingay isway ooray\n","Epoch:  31 | Train loss: 1.093 | Val loss: 1.335 | Gen: etetatatatatatatetat arway ingondinay isway oorway\n","Epoch:  32 | Train loss: 1.082 | Val loss: 1.326 | Gen: etetatatatatatatetat arway ingondinay isway oorway\n","Epoch:  33 | Train loss: 1.071 | Val loss: 1.321 | Gen: etetatatatatatatetat arway ingondinay isway oorway\n","Epoch:  34 | Train loss: 1.062 | Val loss: 1.314 | Gen: etetatatatatatatate arway ingonday isway oorway\n","Epoch:  35 | Train loss: 1.052 | Val loss: 1.306 | Gen: etetatatatatatatate arway ingonday isway ooway\n","Epoch:  36 | Train loss: 1.043 | Val loss: 1.302 | Gen: etetatatatatatatatat arway ingonday isway oorway\n","Epoch:  37 | Train loss: 1.035 | Val loss: 1.295 | Gen: etetatatatatatatatat arway ingonday isway ingpray\n","Epoch:  38 | Train loss: 1.026 | Val loss: 1.290 | Gen: etatetatatatatatatat away ingonday isway ingpray\n","Epoch:  39 | Train loss: 1.018 | Val loss: 1.284 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  40 | Train loss: 1.010 | Val loss: 1.279 | Gen: etatetatatatatatatat away ingonday isway ingpray\n","Epoch:  41 | Train loss: 1.003 | Val loss: 1.274 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  42 | Train loss: 0.996 | Val loss: 1.270 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  43 | Train loss: 0.989 | Val loss: 1.267 | Gen: etatetatatatatatatat away ingonday isway ingpray\n","Epoch:  44 | Train loss: 0.982 | Val loss: 1.262 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  45 | Train loss: 0.976 | Val loss: 1.260 | Gen: etatetatatatatatatat away ingonday isway ingpray\n","Epoch:  46 | Train loss: 0.970 | Val loss: 1.256 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  47 | Train loss: 0.964 | Val loss: 1.253 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  48 | Train loss: 0.958 | Val loss: 1.250 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  49 | Train loss: 0.952 | Val loss: 1.247 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  50 | Train loss: 0.947 | Val loss: 1.245 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  51 | Train loss: 0.941 | Val loss: 1.241 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  52 | Train loss: 0.936 | Val loss: 1.238 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  53 | Train loss: 0.931 | Val loss: 1.237 | Gen: etatetatatatatatatat away ingonday isway ingpray\n","Epoch:  54 | Train loss: 0.927 | Val loss: 1.234 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  55 | Train loss: 0.922 | Val loss: 1.232 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  56 | Train loss: 0.917 | Val loss: 1.230 | Gen: etatetatatatatatatat away ingonday isway ingpray\n","Epoch:  57 | Train loss: 0.913 | Val loss: 1.228 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  58 | Train loss: 0.908 | Val loss: 1.226 | Gen: etatetatatatatatatat away ingonday isway ingpray\n","Epoch:  59 | Train loss: 0.904 | Val loss: 1.224 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  60 | Train loss: 0.900 | Val loss: 1.222 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  61 | Train loss: 0.896 | Val loss: 1.220 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  62 | Train loss: 0.892 | Val loss: 1.218 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  63 | Train loss: 0.888 | Val loss: 1.217 | Gen: etetatatatatatatatat away ingonday isway ingpray\n","Epoch:  64 | Train loss: 0.885 | Val loss: 1.215 | Gen: etetay away ingonday isway ingpray\n","Epoch:  65 | Train loss: 0.881 | Val loss: 1.214 | Gen: etetay away ingonday isway ingpray\n","Epoch:  66 | Train loss: 0.877 | Val loss: 1.212 | Gen: etetay away ingonday isway ingpray\n","Epoch:  67 | Train loss: 0.874 | Val loss: 1.211 | Gen: etetay away ingonday isway ingpray\n","Epoch:  68 | Train loss: 0.870 | Val loss: 1.209 | Gen: etetay away ingontionday isway ingpray\n","Epoch:  69 | Train loss: 0.867 | Val loss: 1.208 | Gen: etetay away ingontionday isway ingpray\n","Epoch:  70 | Train loss: 0.864 | Val loss: 1.206 | Gen: etetay away ingconay isway ingpray\n","Epoch:  71 | Train loss: 0.861 | Val loss: 1.205 | Gen: etetay away ingconay isway ingpray\n","Epoch:  72 | Train loss: 0.857 | Val loss: 1.204 | Gen: etetay away ingcay isway ingpray\n","Epoch:  73 | Train loss: 0.854 | Val loss: 1.202 | Gen: etetay away ingconay isway ingpray\n","Epoch:  74 | Train loss: 0.851 | Val loss: 1.201 | Gen: etetay iray ingcay isway ingpray\n","Epoch:  75 | Train loss: 0.848 | Val loss: 1.201 | Gen: etetay away ingcay isway ingpray\n","Epoch:  76 | Train loss: 0.846 | Val loss: 1.199 | Gen: etetay iray ingcay isway ingpray\n","Epoch:  77 | Train loss: 0.843 | Val loss: 1.198 | Gen: etetay iray ingcay isway ingpray\n","Epoch:  78 | Train loss: 0.840 | Val loss: 1.197 | Gen: etetay iray ingcay isway ingpray\n","Epoch:  79 | Train loss: 0.837 | Val loss: 1.197 | Gen: etetay iray ingcay isway ingpray\n","Epoch:  80 | Train loss: 0.835 | Val loss: 1.195 | Gen: etetay iray ingcay isway ingpray\n","Epoch:  81 | Train loss: 0.832 | Val loss: 1.194 | Gen: ethay iray ingcay isway ingpray\n","Epoch:  82 | Train loss: 0.829 | Val loss: 1.193 | Gen: ethay away ingcay isway ingpray\n","Epoch:  83 | Train loss: 0.827 | Val loss: 1.193 | Gen: ethay away ingcay isway ingpray\n","Epoch:  84 | Train loss: 0.824 | Val loss: 1.192 | Gen: ethay away ingcay isway ingpray\n","Epoch:  85 | Train loss: 0.822 | Val loss: 1.191 | Gen: ethay away ingcay isway ingpray\n","Epoch:  86 | Train loss: 0.820 | Val loss: 1.191 | Gen: ethay away ingcay isway ingpray\n","Epoch:  87 | Train loss: 0.817 | Val loss: 1.190 | Gen: ethay away ingcay isway ingpray\n","Epoch:  88 | Train loss: 0.815 | Val loss: 1.190 | Gen: ethay away ingcay isway ingpray\n","Epoch:  89 | Train loss: 0.813 | Val loss: 1.189 | Gen: ethay away ingcay isway ingpray\n","Epoch:  90 | Train loss: 0.810 | Val loss: 1.187 | Gen: ethay away ingcay isway ingpray\n","Epoch:  91 | Train loss: 0.808 | Val loss: 1.187 | Gen: ethay away ingcay isway ingpray\n","Epoch:  92 | Train loss: 0.806 | Val loss: 1.187 | Gen: ethay away ingcay isway ingpray\n","Epoch:  93 | Train loss: 0.804 | Val loss: 1.185 | Gen: ethay away ingcay isway ingpray\n","Epoch:  94 | Train loss: 0.802 | Val loss: 1.185 | Gen: ethay away ingcay isway ingpray\n","Epoch:  95 | Train loss: 0.800 | Val loss: 1.184 | Gen: ethay away ingcay isway ingpray\n","Epoch:  96 | Train loss: 0.798 | Val loss: 1.184 | Gen: ethay away ingcay isway ingpray\n","Epoch:  97 | Train loss: 0.796 | Val loss: 1.183 | Gen: ethay away ingcay isway ingpray\n","Epoch:  98 | Train loss: 0.794 | Val loss: 1.182 | Gen: ethay away ingcay isway ingpray\n","Epoch:  99 | Train loss: 0.792 | Val loss: 1.182 | Gen: ethay away ingcay isway ingpray\n","Obtained lowest validation loss of: 1.1820401936769485\n","source:\t\tthe air conditioning is working \n","translated:\tethay away ingcay isway ingpray\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","attention_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 100,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 100,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"attention\",\n","    \"decoder_type\": \"attention\",  # options: rnn / rnn_attention / attention / transformer\n","}\n","attention_args_s.update(args_dict)\n","print_opts(attention_args_s)\n","\n","attention_encoder_s, attention_decoder_s, attention_losses_s = train(attention_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, attention_encoder_s, attention_decoder_s, None, attention_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"9tcpUFKqo2Oi"},"source":["## Step 8: Transformer encoder and decoder\n","\n","The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"N3B-fWsarlVk","executionInfo":{"status":"ok","timestamp":1647893922716,"user_tz":240,"elapsed":259,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}}},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n","        super(TransformerEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.self_attentions = nn.ModuleList(\n","            [\n","                ScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.attention_mlps = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.Linear(hidden_size, hidden_size),\n","                    nn.ReLU(),\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","\n","        self.positional_encodings = self.create_positional_encodings()\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            None: Used to conform to standard encoder return signature.\n","            None: Used to conform to standard encoder return signature.\n","        \"\"\"\n","        batch_size, seq_len = inputs.size()\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n","        encoded = encoded + self.positional_encodings[:seq_len]\n","\n","        annotations = encoded\n","        for i in range(self.num_layers):\n","            new_annotations, self_attention_weights = self.self_attentions[i](\n","                annotations, annotations, annotations\n","            )  # batch_size x seq_len x hidden_size\n","            residual_annotations = annotations + new_annotations\n","            new_annotations = self.attention_mlps[i](residual_annotations)\n","            annotations = residual_annotations + new_annotations\n","\n","        # Transformer encoder does not have a last hidden or cell layer.\n","        return annotations, None\n","        # return annotations, None, None\n","    def create_positional_encodings(self, max_seq_len=1000):\n","        \"\"\"Creates positional encodings for the inputs.\n","\n","        Arguments:\n","            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n","\n","        Returns:\n","            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n","        \"\"\"\n","        pos_indices = torch.arange(max_seq_len)[..., None]\n","        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n","        exponents = (2 * dim_indices).float() / (self.hidden_size)\n","        trig_args = pos_indices / (10000**exponents)\n","        sin_terms = torch.sin(trig_args)\n","        cos_terms = torch.cos(trig_args)\n","\n","        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n","        pos_encodings[:, 0::2] = sin_terms\n","        pos_encodings[:, 1::2] = cos_terms\n","\n","        if self.opts.cuda:\n","            pos_encodings = pos_encodings.cuda()\n","\n","        return pos_encodings"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"nyvTZFxtrvc6","executionInfo":{"status":"ok","timestamp":1647893927548,"user_tz":240,"elapsed":244,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}}},"outputs":[],"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers):\n","        super(TransformerDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.num_layers = num_layers\n","\n","        self.self_attentions = nn.ModuleList(\n","            [\n","                CausalScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.encoder_attentions = nn.ModuleList(\n","            [\n","                ScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.attention_mlps = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.Linear(hidden_size, hidden_size),\n","                    nn.ReLU(),\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        self.positional_encodings = self.create_positional_encodings()\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: Not used in the transformer decoder\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        embed = embed + self.positional_encodings[:seq_len]\n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        for i in range(self.num_layers):\n","            new_contexts, self_attention_weights = self.self_attentions[i](\n","                contexts, contexts, contexts\n","            )  # batch_size x seq_len x hidden_size\n","            residual_contexts = contexts + new_contexts\n","            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n","                residual_contexts, annotations, annotations\n","            )  # batch_size x seq_len x hidden_size\n","            residual_contexts = residual_contexts + new_contexts\n","            new_contexts = self.attention_mlps[i](residual_contexts)\n","            contexts = residual_contexts + new_contexts\n","\n","            encoder_attention_weights_list.append(encoder_attention_weights)\n","            self_attention_weights_list.append(self_attention_weights)\n","\n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","\n","        return output, (encoder_attention_weights, self_attention_weights)\n","\n","    def create_positional_encodings(self, max_seq_len=1000):\n","        \"\"\"Creates positional encodings for the inputs.\n","\n","        Arguments:\n","            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n","\n","        Returns:\n","            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n","        \"\"\"\n","        pos_indices = torch.arange(max_seq_len)[..., None]\n","        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n","        exponents = (2 * dim_indices).float() / (self.hidden_size)\n","        trig_args = pos_indices / (10000**exponents)\n","        sin_terms = torch.sin(trig_args)\n","        cos_terms = torch.cos(trig_args)\n","\n","        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n","        pos_encodings[:, 0::2] = sin_terms\n","        pos_encodings[:, 1::2] = cos_terms\n","\n","        pos_encodings = pos_encodings.cuda()\n","\n","        return pos_encodings"]},{"cell_type":"markdown","metadata":{"id":"29ZjkXTNrUKb"},"source":["\n","## Step 9: Training and analysis (with scaled dot-product attention)\n","\n","Now we will train a (simplified) transformer encoder-decoder model.\n","\n","First, we train our smaller model on the small dataset. Use this model to answer Question 5 in the handout."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"mk8e4KSnuZ8N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647894160879,"user_tz":240,"elapsed":209437,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}},"outputId":"16e4f95d-13f6-4d13-8770-ef8b9155472e"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 100                                    \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 32                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 4                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('interfere', 'interfereway')\n","('several', 'everalsay')\n","('hour', 'ourhay')\n","('read', 'eadray')\n","('sends', 'endssay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 3.137 | Val loss: 2.400 | Gen: ay aiay iiisssty iisaay oososay\n","Epoch:   1 | Train loss: 2.147 | Val loss: 2.083 | Gen: attttay ay innslway isay oniontray\n","Epoch:   2 | Train loss: 1.880 | Val loss: 1.961 | Gen: atttay ay onintray isway onisay\n","Epoch:   3 | Train loss: 1.720 | Val loss: 1.934 | Gen: atttay anarway oncntray isay ontray\n","Epoch:   4 | Train loss: 1.615 | Val loss: 1.767 | Gen: etay-ay arrrrray oingoningonininininn isway oonioningay\n","Epoch:   5 | Train loss: 1.500 | Val loss: 1.915 | Gen: eway aray oiongay iway oonionay\n","Epoch:   6 | Train loss: 1.406 | Val loss: 1.747 | Gen: etay-ay aray oinintinay isway ooninay\n","Epoch:   7 | Train loss: 1.319 | Val loss: 1.739 | Gen: etay-ay-ay-ay airway oinintingay isway oininway\n","Epoch:   8 | Train loss: 1.268 | Val loss: 1.652 | Gen: etay-tay arway ointingay isway okingay\n","Epoch:   9 | Train loss: 1.194 | Val loss: 1.692 | Gen: etay-tay arway ointingay isway oninway\n","Epoch:  10 | Train loss: 1.158 | Val loss: 1.630 | Gen: attay-tay arway ociningingingncincin issway okinglay\n","Epoch:  11 | Train loss: 1.110 | Val loss: 1.525 | Gen: etay-tay arrrway otintintingay isway okintray\n","Epoch:  12 | Train loss: 1.028 | Val loss: 1.573 | Gen: etway arway onintingcindayncindn isway okingway\n","Epoch:  13 | Train loss: 0.994 | Val loss: 1.516 | Gen: etway-tttay arrwway onintincincincay isway okintray\n","Epoch:  14 | Train loss: 0.968 | Val loss: 1.513 | Gen: atway arwwwway onintingway isway okingway\n","Epoch:  15 | Train loss: 0.940 | Val loss: 1.497 | Gen: atway-tway arrway onintingay isway okingway\n","Epoch:  16 | Train loss: 0.928 | Val loss: 1.592 | Gen: etway arrwway onintingcinday isway okintray\n","Epoch:  17 | Train loss: 0.887 | Val loss: 1.467 | Gen: attay-tay arway onintingway isway okintway\n","Epoch:  18 | Train loss: 0.818 | Val loss: 1.496 | Gen: hay arirway onininglinday isway okintway\n","Epoch:  19 | Train loss: 0.783 | Val loss: 1.379 | Gen: etway-tay arway onintingoliongnddddd isway okingway\n","Epoch:  20 | Train loss: 0.729 | Val loss: 1.348 | Gen: etway ariway oniningway isway okingway\n","Epoch:  21 | Train loss: 0.708 | Val loss: 1.353 | Gen: ethay arway oncintingongway isway okingway\n","Epoch:  22 | Train loss: 0.690 | Val loss: 1.354 | Gen: ethay arirway oniningway issway okingway\n","Epoch:  23 | Train loss: 0.664 | Val loss: 1.290 | Gen: ethay arwway onintioiongway isway okingway\n","Epoch:  24 | Train loss: 0.634 | Val loss: 1.248 | Gen: ethay arway oniningway isway okingway\n","Epoch:  25 | Train loss: 0.606 | Val loss: 1.252 | Gen: ethay arirway oniningway isway okingway\n","Epoch:  26 | Train loss: 0.615 | Val loss: 1.316 | Gen: ethay arrway oniningcay isway okingway\n","Epoch:  27 | Train loss: 0.603 | Val loss: 1.275 | Gen: ethay airway onininingway isway okingway\n","Epoch:  28 | Train loss: 0.574 | Val loss: 1.303 | Gen: etthay arway ontinioioingcay isway okingway\n","Epoch:  29 | Train loss: 0.553 | Val loss: 1.230 | Gen: ethay airway oninioingway isway okingway\n","Epoch:  30 | Train loss: 0.516 | Val loss: 1.224 | Gen: ethay airway oninioioingway isway okingway\n","Epoch:  31 | Train loss: 0.502 | Val loss: 1.206 | Gen: ethay airway ondinmindiocingway isway okingway\n","Epoch:  32 | Train loss: 0.474 | Val loss: 1.214 | Gen: ethay airway oninioioioingsway isway okingway\n","Epoch:  33 | Train loss: 0.447 | Val loss: 1.150 | Gen: ethay irawway ondiningway isway okingway\n","Epoch:  34 | Train loss: 0.429 | Val loss: 1.124 | Gen: ethay airway ondiningioingsway isway okingway\n","Epoch:  35 | Train loss: 0.418 | Val loss: 1.122 | Gen: ethay iray ondiningway isway okingway\n","Epoch:  36 | Train loss: 0.399 | Val loss: 1.097 | Gen: ethay airway ondiningingway isway okingway\n","Epoch:  37 | Train loss: 0.386 | Val loss: 1.130 | Gen: ethay airway ondiningway isway okingway\n","Epoch:  38 | Train loss: 0.374 | Val loss: 1.112 | Gen: ethay airway ondiningingway isway okingway\n","Epoch:  39 | Train loss: 0.357 | Val loss: 1.135 | Gen: ethay iray ondiningioay isway okingray\n","Epoch:  40 | Train loss: 0.353 | Val loss: 1.137 | Gen: ethay irawway ondinioingway isway owingway\n","Epoch:  41 | Train loss: 0.346 | Val loss: 1.193 | Gen: ethay iray ondinioingway isway okingway\n","Epoch:  42 | Train loss: 0.385 | Val loss: 1.267 | Gen: etthay iraway ondinintiocingway isway okinkrway\n","Epoch:  43 | Train loss: 0.428 | Val loss: 1.347 | Gen: ethay irawway ondininingingcay isway okingray\n","Epoch:  44 | Train loss: 0.430 | Val loss: 1.176 | Gen: ehay-thay irawwway ondiningingway isway okingway\n","Epoch:  45 | Train loss: 0.398 | Val loss: 1.055 | Gen: etthay irawwway ondiningingway isway okingway\n","Epoch:  46 | Train loss: 0.357 | Val loss: 1.059 | Gen: ethay iray ondiningingway isway okingway\n","Epoch:  47 | Train loss: 0.314 | Val loss: 1.061 | Gen: ethay irawwway ondidiningway isway okingway\n","Epoch:  48 | Train loss: 0.286 | Val loss: 1.053 | Gen: ethay irawwway ondidingingway isway okingway\n","Epoch:  49 | Train loss: 0.270 | Val loss: 1.061 | Gen: ethay irawwway ondidingingway isway okingway\n","Epoch:  50 | Train loss: 0.257 | Val loss: 1.071 | Gen: ethay irawwway ondidingingway isway okingway\n","Epoch:  51 | Train loss: 0.248 | Val loss: 1.077 | Gen: ethay irawwway ondidingtingway isway okingway\n","Epoch:  52 | Train loss: 0.240 | Val loss: 1.089 | Gen: ethay irawwway ondidingtingway isway okingway\n","Epoch:  53 | Train loss: 0.231 | Val loss: 1.099 | Gen: ethay irawwway ondidingtingway isway okingway\n","Epoch:  54 | Train loss: 0.223 | Val loss: 1.116 | Gen: ethay irawwway ondidingtingway isway okingray\n","Epoch:  55 | Train loss: 0.215 | Val loss: 1.117 | Gen: ethay irawwway ondidingtingway isway okingray\n","Epoch:  56 | Train loss: 0.208 | Val loss: 1.135 | Gen: ethay irawwway ondidingtingway isway okingray\n","Epoch:  57 | Train loss: 0.201 | Val loss: 1.134 | Gen: ethay irawwway ondidingtingway isway okingray\n","Epoch:  58 | Train loss: 0.196 | Val loss: 1.180 | Gen: ethay irawwway ondidingtingway isway owikrngway\n","Epoch:  59 | Train loss: 0.205 | Val loss: 1.242 | Gen: ethay irawwway ondidiningcay isway owikingway\n","Epoch:  60 | Train loss: 0.248 | Val loss: 1.173 | Gen: ethay irawwway ondiningiocay isway okingray\n","Epoch:  61 | Train loss: 0.322 | Val loss: 1.252 | Gen: ethay ariway ondiningcay isway okrkingway\n","Epoch:  62 | Train loss: 0.336 | Val loss: 1.121 | Gen: ethay irawwwway ondidingingcay isway owingray\n","Epoch:  63 | Train loss: 0.321 | Val loss: 1.062 | Gen: ethay iray ondintingpay isway okingray\n","Epoch:  64 | Train loss: 0.232 | Val loss: 1.026 | Gen: ethay irawway ondintingpay isway owingray\n","Epoch:  65 | Train loss: 0.196 | Val loss: 1.031 | Gen: ethay irawwway ondintingway isway okrkingway\n","Epoch:  66 | Train loss: 0.179 | Val loss: 1.040 | Gen: ethay irawwway ondintingway isway okrkingway\n","Epoch:  67 | Train loss: 0.169 | Val loss: 1.053 | Gen: ethay irawwway ondidintingway isway okrkingway\n","Epoch:  68 | Train loss: 0.161 | Val loss: 1.053 | Gen: ethay irawwway ondidintingway isway okrkingway\n","Epoch:  69 | Train loss: 0.154 | Val loss: 1.056 | Gen: ethay irawwway ondidintingway isway okrkingway\n","Epoch:  70 | Train loss: 0.147 | Val loss: 1.063 | Gen: ethay irawwway ondidintingway isway okrkingway\n","Epoch:  71 | Train loss: 0.141 | Val loss: 1.061 | Gen: ethay irawwway ondidintingway isway okingray\n","Epoch:  72 | Train loss: 0.137 | Val loss: 1.066 | Gen: ethay irawwway ondidintingway isway okrkingway\n","Epoch:  73 | Train loss: 0.131 | Val loss: 1.067 | Gen: ethay irawwway ondidintingway isway okrkingway\n","Epoch:  74 | Train loss: 0.127 | Val loss: 1.071 | Gen: ethay irawway ondidintingway isway okrkingway\n","Epoch:  75 | Train loss: 0.123 | Val loss: 1.074 | Gen: ethay irawwway ondidintingway isway okrkingway\n","Epoch:  76 | Train loss: 0.119 | Val loss: 1.082 | Gen: ethay irawway ondidintingway isway okrkingway\n","Epoch:  77 | Train loss: 0.115 | Val loss: 1.087 | Gen: ethay irawwway ondidintingway isway okrkingway\n","Epoch:  78 | Train loss: 0.111 | Val loss: 1.094 | Gen: ethay iraywway ondidintingcay isway okrkingway\n","Epoch:  79 | Train loss: 0.107 | Val loss: 1.095 | Gen: ethay irawway ondidintingcay isway okringway\n","Epoch:  80 | Train loss: 0.104 | Val loss: 1.102 | Gen: ethay iraywway ondidtingcay isway okringway\n","Epoch:  81 | Train loss: 0.101 | Val loss: 1.106 | Gen: ethay iraywway ondidintingcay isway okringway\n","Epoch:  82 | Train loss: 0.098 | Val loss: 1.117 | Gen: ethay iraywway ondidtiongingway isway okringway\n","Epoch:  83 | Train loss: 0.095 | Val loss: 1.115 | Gen: ethay iraywway ondidtingingcay isway okringway\n","Epoch:  84 | Train loss: 0.093 | Val loss: 1.134 | Gen: ethay iraywway ondidtiongingway isway okringray\n","Epoch:  85 | Train loss: 0.094 | Val loss: 1.153 | Gen: ethay iraywway ondidtiongingway isway okringway\n","Epoch:  86 | Train loss: 0.247 | Val loss: 1.231 | Gen: ethay irway ondibliongtay iisway otingray\n","Epoch:  87 | Train loss: 0.319 | Val loss: 1.289 | Gen: etthway irway ondidingtingoydoy isway owingray\n","Epoch:  88 | Train loss: 0.231 | Val loss: 1.104 | Gen: ethay arwway ondidintingcay isway okringray\n","Epoch:  89 | Train loss: 0.151 | Val loss: 0.982 | Gen: ethay irawway ondintingcay isway okringray\n","Epoch:  90 | Train loss: 0.115 | Val loss: 0.957 | Gen: ethay irawway ondidintingcay isway okringray\n","Epoch:  91 | Train loss: 0.099 | Val loss: 0.968 | Gen: ethay irawway ondidintingcay isway okringray\n","Epoch:  92 | Train loss: 0.092 | Val loss: 0.982 | Gen: ethay irawway ondidintingcay isway okringray\n","Epoch:  93 | Train loss: 0.087 | Val loss: 0.993 | Gen: ethay irawway ondidintingcay isway okringray\n","Epoch:  94 | Train loss: 0.083 | Val loss: 1.003 | Gen: ethay irawway ondidintingcay isway okringray\n","Epoch:  95 | Train loss: 0.079 | Val loss: 1.013 | Gen: ethay irawway ondidintingcay isway okringray\n","Epoch:  96 | Train loss: 0.076 | Val loss: 1.022 | Gen: ethay irawway ondidintingcay isway okringray\n","Epoch:  97 | Train loss: 0.074 | Val loss: 1.030 | Gen: ethay irawway ondidintingcay isway okringray\n","Epoch:  98 | Train loss: 0.071 | Val loss: 1.038 | Gen: ethay irawway ondidintingcay isway okringray\n","Epoch:  99 | Train loss: 0.069 | Val loss: 1.047 | Gen: ethay irawway ondidintingcay isway okringray\n","Obtained lowest validation loss of: 0.9571305304765702\n","source:\t\tthe air conditioning is working \n","translated:\tethay irawway ondidintingcay isway okringray\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans32_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 100,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 100,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 4,\n","}\n","trans32_args_s.update(args_dict)\n","print_opts(trans32_args_s)\n","\n","trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"l28mKuZxvaRT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647894818475,"user_tz":240,"elapsed":258,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}},"outputId":"4d9316f9-c8d0-4b1d-cb04-9c3c798ffaeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe air conditioning is working \n","translated:\tethay irawway ondidintingcay isway okringray\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"0L8EqLYFu48H"},"source":["In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"FdZO69DozuUu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647895004628,"user_tz":240,"elapsed":181237,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}},"outputId":"8d8d6259-fc82-4156-f4cc-719880722ee8"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_large                        \n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 10                                     \n","                               lr_decay: 0.99                                   \n","                             batch_size: 512                                    \n","                            hidden_size: 32                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('scientific', 'ientificscay')\n","('rounds', 'oundsray')\n","('wrapt', 'aptwray')\n","('detract', 'etractday')\n","('amused', 'amusedway')\n","Num unique word pairs: 22402\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.849 | Val loss: 2.273 | Gen: eay ayayay iogmay-ay iiisay oray-ay-ay\n","Epoch:   1 | Train loss: 2.092 | Val loss: 2.044 | Gen: eay away iogmogitay iiiiiiiiiiwwwwwway ormay-ay-ay\n","Epoch:   2 | Train loss: 1.860 | Val loss: 1.904 | Gen: eheheshy away ooooogititincitioiti iwwwwwwwwwwwwwwwwwww ormay-ay\n","Epoch:   3 | Train loss: 1.702 | Val loss: 1.817 | Gen: eheay away oingingay-ingay iwwwwwwwwwwwwwwwwwww ormay-ay\n","Epoch:   4 | Train loss: 1.591 | Val loss: 1.814 | Gen: eheay away oncoingititintititit iwwwwwwwwwwy oggay\n","Epoch:   5 | Train loss: 1.506 | Val loss: 1.668 | Gen: eay away ontingintintintingay iwwwwwwwy orgway-ay\n","Epoch:   6 | Train loss: 1.419 | Val loss: 1.604 | Gen: ethay-ay away ontintintincay iway orkingway\n","Epoch:   7 | Train loss: 1.345 | Val loss: 1.554 | Gen: ethay away ontintintincay iwwy ongway-ay\n","Epoch:   8 | Train loss: 1.278 | Val loss: 1.547 | Gen: ethay away ontingay-inway iway orkingway\n","Epoch:   9 | Train loss: 1.220 | Val loss: 1.538 | Gen: ethay away ondintinay isway orkingway\n","Epoch:  10 | Train loss: 1.171 | Val loss: 1.497 | Gen: ethay arway ontintinay iway orkingway\n","Epoch:  11 | Train loss: 1.131 | Val loss: 1.493 | Gen: ethay away ondingintay isway ooswway\n","Epoch:  12 | Train loss: 1.083 | Val loss: 1.400 | Gen: ethay arway oontinginay isway orkingway\n","Epoch:  13 | Train loss: 1.043 | Val loss: 1.360 | Gen: ethay arway odintinginay isway oosingway\n","Epoch:  14 | Train loss: 0.992 | Val loss: 1.328 | Gen: ethay away odintingicay isay orkingay\n","Epoch:  15 | Train loss: 0.958 | Val loss: 1.310 | Gen: ethay away odintingday isway orkingway\n","Epoch:  16 | Train loss: 0.926 | Val loss: 1.260 | Gen: ethay away odintingway isay onosingway\n","Epoch:  17 | Train loss: 0.904 | Val loss: 1.283 | Gen: ethay arway ondingitingcay isway orkingray\n","Epoch:  18 | Train loss: 0.884 | Val loss: 1.224 | Gen: ethay ariway odinticay isay orkingay\n","Epoch:  19 | Train loss: 0.843 | Val loss: 1.229 | Gen: ethay arway odcintitingway isway orkingway\n","Epoch:  20 | Train loss: 0.814 | Val loss: 1.163 | Gen: ethay ariway odciontitingway isay orkingway\n","Epoch:  21 | Train loss: 0.780 | Val loss: 1.170 | Gen: ethay arway ondicongcay isay orkingay\n","Epoch:  22 | Train loss: 0.757 | Val loss: 1.173 | Gen: ethay ariway odindicatingway isay orkingway\n","Epoch:  23 | Train loss: 0.736 | Val loss: 1.137 | Gen: ethay ariway octingitingway isay orkingray\n","Epoch:  24 | Train loss: 0.730 | Val loss: 1.147 | Gen: ehay ariway odindicatingway isay orkingway\n","Epoch:  25 | Train loss: 0.703 | Val loss: 1.084 | Gen: ethay ariway ondictingway isay orkingray\n","Epoch:  26 | Train loss: 0.676 | Val loss: 1.075 | Gen: ethay ariway oondictingway isay orkingway\n","Epoch:  27 | Train loss: 0.665 | Val loss: 1.041 | Gen: ethay ariway ondictingnay isway orkingray\n","Epoch:  28 | Train loss: 0.663 | Val loss: 1.175 | Gen: ethtway airway oooctingdicay isway orkingway\n","Epoch:  29 | Train loss: 0.667 | Val loss: 1.042 | Gen: ethay iraway odcintiongway isway orkingray\n","Epoch:  30 | Train loss: 0.645 | Val loss: 0.975 | Gen: ethay iray odondictingway isway orkingray\n","Epoch:  31 | Train loss: 0.607 | Val loss: 1.007 | Gen: ethay iraway ondictingnay isway orkingway\n","Epoch:  32 | Train loss: 0.585 | Val loss: 0.974 | Gen: ethay iray odcintiongnay isway orkingrway\n","Epoch:  33 | Train loss: 0.578 | Val loss: 0.959 | Gen: ethtway ariway ondictingnay isway orkingray\n","Epoch:  34 | Train loss: 0.553 | Val loss: 0.947 | Gen: ethway iray ondictingnay isway orkingrway\n","Epoch:  35 | Train loss: 0.528 | Val loss: 1.004 | Gen: ethtway ariway ondictintingway isway orkingrway\n","Epoch:  36 | Train loss: 0.512 | Val loss: 0.949 | Gen: ethway ariway ondictingnay isway orkingrway\n","Epoch:  37 | Train loss: 0.502 | Val loss: 0.993 | Gen: ethway ariway ondictintingway isway orkingray\n","Epoch:  38 | Train loss: 0.493 | Val loss: 0.926 | Gen: ethway arirway ondictingnay isway orkingrway\n","Epoch:  39 | Train loss: 0.480 | Val loss: 0.969 | Gen: ethway ariway ondictintingway isway orkingwway\n","Epoch:  40 | Train loss: 0.467 | Val loss: 0.915 | Gen: ethway ariway ondictingnay isway orkingrway\n","Epoch:  41 | Train loss: 0.458 | Val loss: 0.938 | Gen: ethway ariway ondictintingway isway orkingway\n","Epoch:  42 | Train loss: 0.445 | Val loss: 0.891 | Gen: ethway ariway ondictingnay isway orkingwwwway\n","Epoch:  43 | Train loss: 0.433 | Val loss: 0.938 | Gen: ethway ariway ondictintingway isway orkingwwway\n","Epoch:  44 | Train loss: 0.419 | Val loss: 0.894 | Gen: ethway ariway ondictingway isway orkingwwwway\n","Epoch:  45 | Train loss: 0.409 | Val loss: 0.921 | Gen: ethway ariway ondictintingway isway orkingwway\n","Epoch:  46 | Train loss: 0.400 | Val loss: 0.873 | Gen: ethway airway ocdintingway isway orkingwwway\n","Epoch:  47 | Train loss: 0.391 | Val loss: 0.909 | Gen: ethway airway ondictintingway isway orkingwway\n","Epoch:  48 | Train loss: 0.379 | Val loss: 0.862 | Gen: ethway ariway ondictingway isway orkingwwway\n","Epoch:  49 | Train loss: 0.370 | Val loss: 0.884 | Gen: ethway airway ondictintingway isway orkingway\n","Epoch:  50 | Train loss: 0.361 | Val loss: 0.851 | Gen: ethway airway ocdintiongway isway orkingwway\n","Epoch:  51 | Train loss: 0.353 | Val loss: 0.876 | Gen: ethway airway onditinctingway isway orkingwway\n","Epoch:  52 | Train loss: 0.346 | Val loss: 0.845 | Gen: ethway airway ocdintiongway isway orkingwway\n","Epoch:  53 | Train loss: 0.337 | Val loss: 0.878 | Gen: ethway airway onditinctiongway isway orkingway\n","Epoch:  54 | Train loss: 0.332 | Val loss: 0.841 | Gen: ethway airway ocdintiongway isway orkingwway\n","Epoch:  55 | Train loss: 0.323 | Val loss: 0.849 | Gen: ethway airway onditioncingway isway orkingway\n","Epoch:  56 | Train loss: 0.313 | Val loss: 0.832 | Gen: ethway airway ocdintiongway isway orkingway\n","Epoch:  57 | Train loss: 0.305 | Val loss: 0.844 | Gen: ethway airway onditinctiongway isway orkingway\n","Epoch:  58 | Train loss: 0.300 | Val loss: 0.839 | Gen: ethway airway ocdintiongway isway orkingway\n","Epoch:  59 | Train loss: 0.292 | Val loss: 0.859 | Gen: ethway airway onditioncingway isway orkingway\n","Epoch:  60 | Train loss: 0.286 | Val loss: 0.836 | Gen: ethway airway ocdintiongway isway orkingway\n","Epoch:  61 | Train loss: 0.280 | Val loss: 0.845 | Gen: ethway airway onditinctiongway isway orkingway\n","Epoch:  62 | Train loss: 0.272 | Val loss: 0.825 | Gen: ethway airway onditiongcay isway orkingway\n","Epoch:  63 | Train loss: 0.267 | Val loss: 0.842 | Gen: ethway airway onditioncingway isway orkingway\n","Epoch:  64 | Train loss: 0.262 | Val loss: 0.873 | Gen: ethway airway onditioncingway isway orkingway\n","Epoch:  65 | Train loss: 0.279 | Val loss: 0.999 | Gen: ethay airway onditinctiongway isway orkingway\n","Epoch:  66 | Train loss: 0.374 | Val loss: 0.982 | Gen: ethayway iraway ocintiongway isway orkingwwway\n","Epoch:  67 | Train loss: 0.383 | Val loss: 0.946 | Gen: ethay awirway ondointictingway isway orkwongway\n","Epoch:  68 | Train loss: 0.327 | Val loss: 0.792 | Gen: ethway airway onditiongcay isway orkingway\n","Epoch:  69 | Train loss: 0.281 | Val loss: 0.808 | Gen: ethway airway onditiongcay isway orkingway\n","Epoch:  70 | Train loss: 0.262 | Val loss: 0.756 | Gen: ethway airway onditioncingway isway orkingway\n","Epoch:  71 | Train loss: 0.244 | Val loss: 0.766 | Gen: ethway airway onditiongcay isway orkingway\n","Epoch:  72 | Train loss: 0.235 | Val loss: 0.737 | Gen: ethway airway onditioningcay isway orkingway\n","Epoch:  73 | Train loss: 0.225 | Val loss: 0.739 | Gen: ethway airway onditiongnay isway orkingway\n","Epoch:  74 | Train loss: 0.220 | Val loss: 0.729 | Gen: ethway airway onditiongnay isway orkingway\n","Epoch:  75 | Train loss: 0.214 | Val loss: 0.739 | Gen: ethway airway onditioningcay isway orkingway\n","Epoch:  76 | Train loss: 0.212 | Val loss: 0.732 | Gen: ethay airway onditiongnay isway orkingway\n","Epoch:  77 | Train loss: 0.208 | Val loss: 0.763 | Gen: ethway airway onditioningcay isway orkingway\n","Epoch:  78 | Train loss: 0.217 | Val loss: 0.739 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  79 | Train loss: 0.210 | Val loss: 0.771 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  80 | Train loss: 0.205 | Val loss: 0.740 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  81 | Train loss: 0.199 | Val loss: 0.751 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  82 | Train loss: 0.200 | Val loss: 0.742 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  83 | Train loss: 0.193 | Val loss: 0.750 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  84 | Train loss: 0.187 | Val loss: 0.730 | Gen: ethay airway onditioningcay isway orkingway\n","Validation loss has not improved in 10 epochs, stopping early\n","Obtained lowest validation loss of: 0.7289705030464878\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditioningcay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans32_args_l = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n","    \"cuda\": True,\n","    \"nepochs\": 100,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 10,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 512,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 3,\n","}\n","trans32_args_l.update(args_dict)\n","print_opts(trans32_args_l)\n","\n","trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"SmoTgrDcr_dw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647895252341,"user_tz":240,"elapsed":86638,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}},"outputId":"c7f61029-3f65-4508-f6a4-6e3ceff4797e"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 20                                     \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 64                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('interfere', 'interfereway')\n","('several', 'everalsay')\n","('hour', 'ourhay')\n","('read', 'eadray')\n","('sends', 'endssay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.574 | Val loss: 2.020 | Gen: ay aray onday-ay-ay issay ongay-ongay-ongay-on\n","Epoch:   1 | Train loss: 1.762 | Val loss: 1.770 | Gen: ay-ay-ay-ay away onongay-onay iway ororay-oway\n","Epoch:   2 | Train loss: 1.495 | Val loss: 1.681 | Gen: athay ay otiontionationationt iway oway-iongway\n","Epoch:   3 | Train loss: 1.321 | Val loss: 1.564 | Gen: thththy iray indodationdgionay isway oongray-ingngngway\n","Epoch:   4 | Train loss: 1.189 | Val loss: 1.583 | Gen: away iway inononiontiontingngn iway owangway-ingngngngng\n","Epoch:   5 | Train loss: 1.083 | Val loss: 1.534 | Gen: ethay-ehay iraway ocicicationonationgi isway orgrway-ioway-eway-e\n","Epoch:   6 | Train loss: 0.961 | Val loss: 1.419 | Gen: eway arway onditiongway isway owwwway-ingway\n","Epoch:   7 | Train loss: 0.873 | Val loss: 1.428 | Gen: ethway arway ondtitingngnay isway orwwway-ingway\n","Epoch:   8 | Train loss: 0.816 | Val loss: 1.291 | Gen: eway away ondiodiontiontinay isway owwwwway-ingway\n","Epoch:   9 | Train loss: 0.767 | Val loss: 1.228 | Gen: ethty irrway ongiciongingtnay isway orgingway\n","Epoch:  10 | Train loss: 0.658 | Val loss: 1.069 | Gen: ethay arway onditiongtionay isway owgingway\n","Epoch:  11 | Train loss: 0.554 | Val loss: 1.044 | Gen: ethay arway ondititiongtngtnay isway oringingway\n","Epoch:  12 | Train loss: 0.487 | Val loss: 1.043 | Gen: ethtyway arway onditiongiongtngnay isway okingingway\n","Epoch:  13 | Train loss: 0.446 | Val loss: 1.033 | Gen: ethtay arway onditititiongtngngng isway orgingingway\n","Epoch:  14 | Train loss: 0.422 | Val loss: 0.903 | Gen: eethty airway onditiongiongtcay isway owgingway\n","Epoch:  15 | Train loss: 0.366 | Val loss: 0.936 | Gen: ethttyay arway onditiongiongtngnay isway okingray\n","Epoch:  16 | Train loss: 0.337 | Val loss: 0.905 | Gen: ehtay arway onditiongctingtay isy okingbay\n","Epoch:  17 | Train loss: 0.341 | Val loss: 0.934 | Gen: ethtttythay arrway onditiongcgcay isway okiggngray\n","Epoch:  18 | Train loss: 0.331 | Val loss: 0.830 | Gen: ethay arway onditiongcay isway okingray\n","Epoch:  19 | Train loss: 0.262 | Val loss: 0.835 | Gen: ethtytay arway onditiontingcay isway owkingway\n","Epoch:  20 | Train loss: 0.227 | Val loss: 0.751 | Gen: ethay arway onditiongcay isway owkingway\n","Epoch:  21 | Train loss: 0.192 | Val loss: 0.688 | Gen: ethtyay arway onditiongcay isway okingray\n","Epoch:  22 | Train loss: 0.161 | Val loss: 0.710 | Gen: ethtay arway onditiongcay isway owkingway\n","Epoch:  23 | Train loss: 0.149 | Val loss: 0.740 | Gen: ethtay arway onditiongcay isway owgingway\n","Epoch:  24 | Train loss: 0.144 | Val loss: 0.736 | Gen: ethtay arway onditioncgcay isway owringway\n","Epoch:  25 | Train loss: 0.164 | Val loss: 0.809 | Gen: ethay arway onditioncangcay isway owangingway\n","Epoch:  26 | Train loss: 0.171 | Val loss: 0.821 | Gen: ethtay arway onditioncangcay isway owringway\n","Epoch:  27 | Train loss: 0.187 | Val loss: 0.669 | Gen: ethtay ariway onditiongcay isway owringway\n","Epoch:  28 | Train loss: 0.162 | Val loss: 0.787 | Gen: ethtay arrway onditiongcay isway okrkingway\n","Epoch:  29 | Train loss: 0.152 | Val loss: 0.703 | Gen: ethtyay arway onditiongcay isway orkingway\n","Epoch:  30 | Train loss: 0.138 | Val loss: 0.604 | Gen: ethay ariway onditiongiongcay isway owringway\n","Epoch:  31 | Train loss: 0.104 | Val loss: 0.619 | Gen: ethtay ariway onditiongcay isway okingray\n","Epoch:  32 | Train loss: 0.095 | Val loss: 0.581 | Gen: ethtay ariway onditiongiongcay isway owkingray\n","Epoch:  33 | Train loss: 0.074 | Val loss: 0.575 | Gen: ethay arway onditiongcay isway owkingway\n","Epoch:  34 | Train loss: 0.061 | Val loss: 0.524 | Gen: ethtay ariway onditiongcay isway orkingway\n","Epoch:  35 | Train loss: 0.054 | Val loss: 0.510 | Gen: ethay ariway onditiongiongcay isway orkingway\n","Epoch:  36 | Train loss: 0.060 | Val loss: 0.551 | Gen: ethay airway onditiongiongcay isway orkingngway\n","Epoch:  37 | Train loss: 0.076 | Val loss: 0.737 | Gen: ethay ariway onditiongcay isway orkingway\n","Epoch:  38 | Train loss: 0.093 | Val loss: 0.639 | Gen: ethay ariway onditiongiongcay isway orkingway\n","Epoch:  39 | Train loss: 0.084 | Val loss: 0.584 | Gen: ethay ariway onditingday isway okringnay\n","Epoch:  40 | Train loss: 0.053 | Val loss: 0.515 | Gen: ethay ariway onditiongingcay isway orkingway\n","Epoch:  41 | Train loss: 0.038 | Val loss: 0.541 | Gen: ethay airway onditiongingcay isway orkingnay\n","Epoch:  42 | Train loss: 0.034 | Val loss: 0.507 | Gen: ethay airway onditiongiongcay isway orkingway\n","Epoch:  43 | Train loss: 0.031 | Val loss: 0.475 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  44 | Train loss: 0.026 | Val loss: 0.578 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  45 | Train loss: 0.193 | Val loss: 1.277 | Gen: ethay awarrway ooditingongcay iway okringnglay\n","Epoch:  46 | Train loss: 0.291 | Val loss: 0.691 | Gen: ethty airway ondidiontingcay isdway owingingway\n","Epoch:  47 | Train loss: 0.143 | Val loss: 0.841 | Gen: ethay arway onditioniongcay issway okrkingway\n","Epoch:  48 | Train loss: 0.127 | Val loss: 0.514 | Gen: ethtay iriway onditiongcay isway orkingway\n","Epoch:  49 | Train loss: 0.060 | Val loss: 0.444 | Gen: ethtay airway onditiongcay isway okingway\n","Obtained lowest validation loss of: 0.44411674374714494\n","source:\t\tthe air conditioning is working \n","translated:\tethtay airway onditiongcay isway okingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans64_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 20,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 64,  # Increased model size\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 3,\n","}\n","trans64_args_s.update(args_dict)\n","print_opts(trans64_args_s)\n","\n","trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"dardK4RWvUWV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647895425826,"user_tz":240,"elapsed":104684,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}},"outputId":"102ddaf9-5c72-40f7-9914-47157be3744a"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_large                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 20                                     \n","                               lr_decay: 0.99                                   \n","                             batch_size: 512                                    \n","                            hidden_size: 64                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('scientific', 'ientificscay')\n","('rounds', 'oundsray')\n","('wrapt', 'aptwray')\n","('detract', 'etractday')\n","('amused', 'amusedway')\n","Num unique word pairs: 22402\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.396 | Val loss: 1.954 | Gen: etetetetetetay ay ontinnngay way oooray\n","Epoch:   1 | Train loss: 1.709 | Val loss: 1.741 | Gen: etay-tay away oontingay ussssssssssay oooooooooooooooooooo\n","Epoch:   2 | Train loss: 1.497 | Val loss: 1.559 | Gen: etay away ontingay isway orway\n","Epoch:   3 | Train loss: 1.318 | Val loss: 1.542 | Gen: etay-tay away ontinngngway isway orwngway\n","Epoch:   4 | Train loss: 1.215 | Val loss: 1.410 | Gen: ethehay iray ontingway isssway orwinway\n","Epoch:   5 | Train loss: 1.077 | Val loss: 1.421 | Gen: ethay ariway ontingay isway ongway\n","Epoch:   6 | Train loss: 0.973 | Val loss: 1.266 | Gen: ehay away ongay isway oghingway\n","Epoch:   7 | Train loss: 0.905 | Val loss: 1.301 | Gen: ethay away ootingway isway oway\n","Epoch:   8 | Train loss: 0.855 | Val loss: 1.129 | Gen: ethay irayrrray ontingway iswayss owingray\n","Epoch:   9 | Train loss: 0.727 | Val loss: 1.125 | Gen: ethay irray ointingway isway oigringway\n","Epoch:  10 | Train loss: 0.668 | Val loss: 1.014 | Gen: ethay iray ondintingway isway owingway\n","Epoch:  11 | Train loss: 0.590 | Val loss: 0.891 | Gen: ethay iray ondintingway isway owingway\n","Epoch:  12 | Train loss: 0.550 | Val loss: 0.954 | Gen: ehay iray oondintingway isway oowingray\n","Epoch:  13 | Train loss: 0.515 | Val loss: 0.835 | Gen: ehay irway ondintingway isway okingway\n","Epoch:  14 | Train loss: 0.435 | Val loss: 0.858 | Gen: ehtway irway ondintingcay isway orkingway\n","Epoch:  15 | Train loss: 0.388 | Val loss: 0.788 | Gen: ehtway arirway ondictingcay isway orkingway\n","Epoch:  16 | Train loss: 0.352 | Val loss: 0.889 | Gen: ehtway irway onditincongcay isway orkingway\n","Epoch:  17 | Train loss: 0.351 | Val loss: 0.800 | Gen: ethway irway ondictingway isway orkingway\n","Epoch:  18 | Train loss: 0.300 | Val loss: 0.772 | Gen: ehay irway onditioncay isway orkingway\n","Epoch:  19 | Train loss: 0.293 | Val loss: 0.952 | Gen: ehtway irway onditiongcingway isway orrkingway\n","Epoch:  20 | Train loss: 0.323 | Val loss: 0.764 | Gen: ethway ariway ondioncodingcay isway orkingway\n","Epoch:  21 | Train loss: 0.312 | Val loss: 0.846 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  22 | Train loss: 0.290 | Val loss: 0.844 | Gen: ehtway arway onditingcingway isway orkinggwgway\n","Epoch:  23 | Train loss: 0.315 | Val loss: 0.701 | Gen: ehay arway onditiongcingcay isway orkingway\n","Epoch:  24 | Train loss: 0.314 | Val loss: 0.823 | Gen: ehtway irway oonditiongngcay isway orkingway\n","Epoch:  25 | Train loss: 0.322 | Val loss: 0.750 | Gen: ehtway arway onditincay isway owingway\n","Epoch:  26 | Train loss: 0.244 | Val loss: 0.476 | Gen: ehtway airway onditiongcingway isway orkingway\n","Epoch:  27 | Train loss: 0.164 | Val loss: 0.451 | Gen: ehtway airway onditioningcay isway orkingway\n","Epoch:  28 | Train loss: 0.132 | Val loss: 0.444 | Gen: ehtway airway onditiongingcay isway orkingway\n","Epoch:  29 | Train loss: 0.118 | Val loss: 0.456 | Gen: ehtway airway onditioningcay isway orkingway\n","Epoch:  30 | Train loss: 0.104 | Val loss: 0.433 | Gen: ehtway airwawway onditioningcingcay isway orkingway\n","Epoch:  31 | Train loss: 0.100 | Val loss: 0.436 | Gen: ethay airwaway onditioningcay isway orkingwlay\n","Epoch:  32 | Train loss: 0.091 | Val loss: 0.403 | Gen: ehtay airwawway onditioningcay isway orkingway\n","Epoch:  33 | Train loss: 0.087 | Val loss: 0.510 | Gen: ethway airwaway onditioningnay isway orkingwlay\n","Epoch:  34 | Train loss: 0.085 | Val loss: 0.460 | Gen: ehtay airwaway onditioningcay isway orkingway\n","Epoch:  35 | Train loss: 0.082 | Val loss: 0.510 | Gen: ethway airwawway onditioningcay isway orkingway\n","Epoch:  36 | Train loss: 0.078 | Val loss: 0.483 | Gen: ethay airwaway onditioningngcay isway orkingwlay\n","Epoch:  37 | Train loss: 0.075 | Val loss: 0.445 | Gen: ethway airway onditioningcay isway orkingway\n","Epoch:  38 | Train loss: 0.115 | Val loss: 0.648 | Gen: ethway airway onditioningcay isway orkingway\n","Epoch:  39 | Train loss: 0.215 | Val loss: 0.903 | Gen: ehtway airway ondiiotingcingway iway orkiinwwway\n","Epoch:  40 | Train loss: 0.262 | Val loss: 0.558 | Gen: ehwthay airway onditionongcay isway orkingway\n","Epoch:  41 | Train loss: 0.150 | Val loss: 0.516 | Gen: ehwtay airway onditioningcay isway orkingway\n","Epoch:  42 | Train loss: 0.115 | Val loss: 0.420 | Gen: ethay airwaway onditioningcay isway orkingway\n","Epoch:  43 | Train loss: 0.076 | Val loss: 0.419 | Gen: ethway airway onditioningcay isway orkingway\n","Epoch:  44 | Train loss: 0.060 | Val loss: 0.364 | Gen: ethay airwawway onditioningcay isway orkingway\n","Epoch:  45 | Train loss: 0.054 | Val loss: 0.387 | Gen: eehtay airway onditioningcay isway orkingway\n","Epoch:  46 | Train loss: 0.053 | Val loss: 0.338 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  47 | Train loss: 0.041 | Val loss: 0.374 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  48 | Train loss: 0.036 | Val loss: 0.360 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  49 | Train loss: 0.033 | Val loss: 0.374 | Gen: ethway airway onditioningcay isway orkingway\n","Obtained lowest validation loss of: 0.3379830677634648\n","source:\t\tthe air conditioning is working \n","translated:\tethway airway onditioningcay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans64_args_l = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 20,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 512,\n","    \"hidden_size\": 64,  # Increased model size\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 3,\n","}\n","trans64_args_l.update(args_dict)\n","print_opts(trans64_args_l)\n","\n","trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"pSSyiG39vVlN"},"source":["The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"-Ql0pxrEvVP6","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1647895646699,"user_tz":240,"elapsed":1451,"user":{"displayName":"Patrick Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT3KM-yqViBms_p8fJH0TcCtFwUNGOB9JemUsZ=s64","userId":"05993880355348232487"}},"outputId":"0ee9cd5e-768f-4a93-e3e8-34f80b8ac9b2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}],"source":["save_loss_comparison_by_dataset(\n","    trans32_losses_s,\n","    trans32_losses_l,\n","    trans64_losses_s,\n","    trans64_losses_l,\n","    trans32_args_s,\n","    trans32_args_l,\n","    trans64_args_s,\n","    trans64_args_l,\n","    \"trans_by_dataset\",\n",")\n","save_loss_comparison_by_hidden(\n","    trans32_losses_s,\n","    trans32_losses_l,\n","    trans64_losses_s,\n","    trans64_losses_l,\n","    trans32_args_s,\n","    trans32_args_l,\n","    trans64_args_s,\n","    trans64_args_l,\n","    \"trans_by_hidden\",\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TjPTaRB4mpCd","s9IS9B9-yUU5","9DaTdRNuUra7","4BIpGwANoQOg","pbvpn4MaV0I1","bRWfRdmVVjUl","0yh08KhgnA30","TSDTbsydlaGI","ZkjHbtvT6Qxs","B7gJLw5t_rnW","9tcpUFKqo2Oi","29ZjkXTNrUKb"],"name":"nmt.ipynb","provenance":[{"file_id":"https://github.com/uoft-csc413/2022/blob/master/assets/assignments/nmt.ipynb","timestamp":1647830542311}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}